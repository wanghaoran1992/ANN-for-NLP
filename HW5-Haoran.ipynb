{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW5: A GRU-pair model for SNLI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment we'll build train a GRU RNN-based model for SNLI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll need to download and unzip SNLI, which you can find [here](http://nlp.stanford.edu/projects/snli/). Set `snli_home` below to point to it. The following block of code loads it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "snli_home = '../snli_1.0'\n",
    "\n",
    "import re\n",
    "import random\n",
    "import json\n",
    "\n",
    "LABEL_MAP = {\n",
    "    \"entailment\": 0,\n",
    "    \"neutral\": 1,\n",
    "    \"contradiction\": 2\n",
    "}\n",
    "\n",
    "def load_snli_data(path):\n",
    "    data = []\n",
    "    with open(path) as f:\n",
    "        for line in f:\n",
    "            loaded_example = json.loads(line)\n",
    "            if loaded_example[\"gold_label\"] not in LABEL_MAP:\n",
    "                continue\n",
    "            loaded_example[\"label\"] = LABEL_MAP[loaded_example[\"gold_label\"]]\n",
    "            data.append(loaded_example)\n",
    "        random.seed(1)\n",
    "        random.shuffle(data)\n",
    "    return data\n",
    "     \n",
    "training_set = load_snli_data(snli_home + '/snli_1.0_train.jsonl')\n",
    "dev_set = load_snli_data(snli_home + '/snli_1.0_dev.jsonl')\n",
    "test_set = load_snli_data(snli_home + '/snli_1.0_test.jsonl')\n",
    "\n",
    "# Note: Unlike with k-nearest neighbors, evaluation here should be fast, and we don't need to\n",
    "# trim down the dev and test sets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll convert the data to index vectors in the same way that we've done for in-class exercises with RNN-based sentiment models. A few notes:\n",
    "\n",
    "- We use a sequence length of only 10, which is short enough that we're truncating a large fraction of sentences.\n",
    "- Tokenization is easy here because we're relying on the output of a parser (which does tokenization as part of parsing), just as with the SST corpus that we've been using until now. Note that we use the 'sentence1_binary_parse' field of each example rather than the human-readable 'sentence1'.\n",
    "- We're using a moderately large vocabulary (for a class exercise) of about 12k words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SEQ_LEN = 10\n",
    "\n",
    "import collections\n",
    "import numpy as np\n",
    "\n",
    "def sentences_to_padded_index_sequences(datasets):\n",
    "    '''Annotates datasets with feature vectors.'''\n",
    "    \n",
    "    PADDING = \"<PAD>\"\n",
    "    UNKNOWN = \"<UNK>\"\n",
    "    \n",
    "    # Extract vocabulary\n",
    "    def tokenize(string):\n",
    "        string = re.sub(r'\\(|\\)', '', string)\n",
    "        return string.lower().split()\n",
    "    \n",
    "    word_counter = collections.Counter()\n",
    "    for example in datasets[0]:\n",
    "        word_counter.update(tokenize(example['sentence1_binary_parse']))\n",
    "        word_counter.update(tokenize(example['sentence2_binary_parse']))\n",
    "        \n",
    "    vocabulary = set([word for word in word_counter if word_counter[word] > 10])\n",
    "    vocabulary = list(vocabulary)\n",
    "    vocabulary = [PADDING, UNKNOWN] + vocabulary\n",
    "        \n",
    "    word_indices = dict(zip(vocabulary, range(len(vocabulary))))\n",
    "    indices_to_words = {v: k for k, v in word_indices.items()}\n",
    "        \n",
    "    for i, dataset in enumerate(datasets):\n",
    "        for example in dataset:\n",
    "            for sentence in ['sentence1_binary_parse', 'sentence2_binary_parse']:\n",
    "                example[sentence + '_index_sequence'] = np.zeros((SEQ_LEN), dtype=np.int32)\n",
    "\n",
    "                token_sequence = tokenize(example[sentence])\n",
    "                padding = SEQ_LEN - len(token_sequence)\n",
    "\n",
    "                for i in range(SEQ_LEN):\n",
    "                    if i >= padding:\n",
    "                        if token_sequence[i - padding] in word_indices:\n",
    "                            index = word_indices[token_sequence[i - padding]]\n",
    "                        else:\n",
    "                            index = word_indices[UNKNOWN]\n",
    "                    else:\n",
    "                        index = word_indices[PADDING]\n",
    "                    example[sentence + '_index_sequence'][i] = index\n",
    "    return indices_to_words, word_indices\n",
    "    \n",
    "indices_to_words, word_indices = sentences_to_padded_index_sequences([training_set, dev_set, test_set])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'annotator_labels': [u'contradiction'], u'sentence2_parse': u'(ROOT (NP (NP (NNS People)) (PP (IN on) (NP (DT a) (NN bike))) (PP (IN on) (NP (DT a) (NN beach))) (. .)))', u'sentence1_binary_parse': u'( ( Five men ) ( ( are ( ( ( playing ( musical instruments ) ) together ) ( on ( a stage ) ) ) ) . ) )', u'captionID': u'2430018178.jpg#1', 'sentence1_binary_parse_index_sequence': array([ 6060, 11028,  7751,  3574, 10502,  3750, 10482,  4668,  2910,   472], dtype=int32), 'label': 2, u'sentence2_binary_parse': u'( ( ( People ( on ( a bike ) ) ) ( on ( a beach ) ) ) . )', u'pairID': u'2430018178.jpg#1r1c', u'sentence2': u'People on a bike on a beach.', u'sentence1_parse': u'(ROOT (S (NP (CD Five) (NNS men)) (VP (VBP are) (VP (VBG playing) (NP (JJ musical) (NNS instruments)) (ADVP (RB together)) (PP (IN on) (NP (DT a) (NN stage))))) (. .)))', 'sentence2_binary_parse_index_sequence': array([    0,     0,   592, 10482,  4668,  3030, 10482,  4668,  3180,   472], dtype=int32), u'gold_label': u'contradiction', u'sentence1': u'Five men are playing musical instruments together on a stage.'}\n",
      "12089\n"
     ]
    }
   ],
   "source": [
    "print training_set[6]\n",
    "print len(word_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we load GloVe. You'll need the same file that you used for the in-class exercise on word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "glove_home = '../'\n",
    "words_to_load = 25000\n",
    "\n",
    "with open(glove_home + 'glove.6B.50d.txt') as f:\n",
    "    loaded_embeddings = np.zeros((len(word_indices), 50), dtype='float32')\n",
    "    for i, line in enumerate(f):\n",
    "        if i >= words_to_load: \n",
    "            break\n",
    "        \n",
    "        s = line.split()\n",
    "        if s[0] in word_indices:\n",
    "            loaded_embeddings[word_indices[s[0]], :] = np.asarray(s[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we set up an evaluation function as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def evaluate_classifier(classifier, eval_set):\n",
    "    correct = 0\n",
    "    hypotheses = classifier(eval_set)\n",
    "    for i, example in enumerate(eval_set):\n",
    "        hypothesis = hypotheses[i]\n",
    "        if hypothesis == example['label']:\n",
    "            correct += 1        \n",
    "    return correct / float(len(eval_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Implementation (70%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expand the below starter code to build a GRU RNN-pair NLI model. The model should feature the following:\n",
    "\n",
    "- 50D word embeddings initialized with GloVe and trained. (Using self.E should provide this.)\n",
    "- Two GRUs (sharing one set of parameters) that read each sentence independantly and produce one $\\vec{h}_t$ vector for each. We'll call these two vectors $\\vec{h}_{p}$ for the premise (first sentence) and $\\vec{h}_{h}$ for the hypothesis (second sentence).\n",
    "- A combination layer in the style of [Mou et al. 15](https://arxiv.org/abs/1512.08422)'s heuristic matching layer: A ReLU layer with the following four vectors as inputs:\n",
    "  - $\\vec{h}_p$, $\\vec{h}_h$, $\\vec{h}_p - \\vec{h}_h$, $\\vec{h}_p * \\vec{h}_h$\n",
    "- A three-way softmax classifier whose inputs are the outputs of the combination layer.\n",
    "\n",
    "As in the previous assignment, you may use code from in-class exercises, but you should not use any specialized TF functions for LSTMs or RNNs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class RNNEntailmentClassifier:\n",
    "    def __init__(self, vocab_size, sequence_length):\n",
    "        # Define the hyperparameters\n",
    "        self.learning_rate = 0.85  # Should be about right\n",
    "        self.training_epochs = 25  # How long to train for - chosen to fit within class time\n",
    "        self.display_epoch_freq = 1  # How often to test and print out statistics\n",
    "        self.dim = 16  # The dimension of the hidden state of the RNN\n",
    "        self.combination_dim = 32  # The dimension of the hidden state of the combination layer\n",
    "        self.embedding_dim = 50  # The dimension of the learned word embeddings\n",
    "        self.batch_size = 256  # Somewhat arbitrary - can be tuned, but often tune for speed, not accuracy\n",
    "        self.vocab_size = vocab_size  # Defined by the file reader above\n",
    "        self.sequence_length = sequence_length  # Defined by the file reader above\n",
    "        \n",
    "        # Define the parameters\n",
    "        self.E = tf.Variable(loaded_embeddings)\n",
    "        \n",
    "        self.W_cl = tf.Variable(tf.random_normal([self.combination_dim, 3], stddev=0.1))\n",
    "        self.b_cl = tf.Variable(tf.random_normal([3], stddev=0.1))\n",
    "        \n",
    "        # Define the rest of the parameters\n",
    "        self.W_rnn = tf.Variable(tf.random_normal([self.embedding_dim + self.dim, self.dim], stddev=0.1))\n",
    "        self.b_rnn = tf.Variable(tf.random_normal([self.dim], stddev=0.1))\n",
    "        \n",
    "        self.W_r = tf.Variable(tf.random_normal([self.embedding_dim + self.dim, self.dim], stddev=0.1))\n",
    "        self.b_r = tf.Variable(tf.random_normal([self.dim], stddev=0.1))\n",
    "        \n",
    "        self.W_z = tf.Variable(tf.random_normal([self.embedding_dim + self.dim, self.dim], stddev=0.1))\n",
    "        self.b_z = tf.Variable(tf.random_normal([self.dim], stddev=0.1))\n",
    "        \n",
    "        # parameter for the combination layer\n",
    "        self.combination = tf.Variable(tf.random_normal([64, self.combination_dim]))\n",
    "        # Define the placeholders\n",
    "        self.premise_x = tf.placeholder(tf.int32, [None, self.sequence_length])\n",
    "        self.hypothesis_x = tf.placeholder(tf.int32, [None, self.sequence_length])\n",
    "        self.y = tf.placeholder(tf.int32, [None])\n",
    "        \n",
    "        # Split up the inputs into individual tensors\n",
    "        self.x_premise_slices = tf.split(1, self.sequence_length, self.premise_x)\n",
    "        self.x_hypothesis_slices = tf.split(1, self.sequence_length, self.hypothesis_x)\n",
    "        \n",
    "        # Define one step of the RNN\n",
    "        def step(x, h_prev):\n",
    "            emb = tf.nn.embedding_lookup(self.E, x)\n",
    "            emb_h_prev = tf.concat(1, [emb, h_prev])\n",
    "            z = tf.nn.sigmoid(tf.matmul(emb_h_prev, self.W_z) + self.b_z)\n",
    "            r = tf.nn.sigmoid(tf.matmul(emb_h_prev, self.W_r) + self.b_r)\n",
    "            emb_r_h_prev = tf.concat(1, [emb, r * h_prev])\n",
    "            h_tilde = tf.nn.tanh(tf.matmul(emb_r_h_prev, self.W_rnn) + self.b_rnn)\n",
    "            h = (1. - z) * h_prev + z * h_tilde\n",
    "            return h\n",
    "        \n",
    "        self.h_zero = tf.zeros(tf.pack([tf.shape(self.premise_x)[0], self.dim]))\n",
    "        h_prev1 = self.h_zero\n",
    "        # Unroll the first RNN\n",
    "        for t in range(self.sequence_length):\n",
    "            x_t1 = tf.reshape(self.x_premise_slices[t], [-1])\n",
    "            h_prev1 = step(x_t1, h_prev1)\n",
    "        \n",
    "        # Unroll the second RNN\n",
    "        h_prev2 = tf.zeros(tf.pack([tf.shape(self.hypothesis_x)[0], self.dim]))\n",
    "        for t in range(self.sequence_length):\n",
    "            x_t2 = tf.reshape(self.x_hypothesis_slices[t], [-1])\n",
    "            h_prev2 = step(x_t2, h_prev2)\n",
    "        # Build the combination layer\n",
    "        combination_in = tf.concat(1, [h_prev1, h_prev2, tf.sub(h_prev1, h_prev2), tf.mul(h_prev1, h_prev2)])\n",
    "        combination_out = tf.nn.relu(tf.matmul(combination_in, self.combination))\n",
    "        # Compute the logits\n",
    "        self.logits = tf.matmul(combination_out, self.W_cl) + self.b_cl\n",
    "        \n",
    "        # Define the cost function (here, the softmax exp and sum are built in)\n",
    "        self.total_cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(self.logits, self.y))\n",
    "        \n",
    "        # This  performs the main SGD update equation with gradient clipping\n",
    "        optimizer_obj = tf.train.GradientDescentOptimizer(learning_rate=self.learning_rate)\n",
    "        gvs = optimizer_obj.compute_gradients(self.total_cost)\n",
    "        capped_gvs = [(tf.clip_by_norm(grad, 5.0), var) for grad, var in gvs if grad is not None]\n",
    "        self.optimizer = optimizer_obj.apply_gradients(capped_gvs)\n",
    "        \n",
    "        # Create an operation to fill zero values in for W and b\n",
    "        self.init = tf.initialize_all_variables()\n",
    "        \n",
    "        # Create a placeholder for the session that will be shared between training and evaluation\n",
    "        self.sess = None\n",
    "        \n",
    "    def train(self, training_data, dev_data):\n",
    "        def get_minibatch(dataset, start_index, end_index):\n",
    "            indices = range(start_index, end_index)\n",
    "            premise_vectors = np.vstack([dataset[i]['sentence1_binary_parse_index_sequence'] for i in indices])\n",
    "            hypothesis_vectors = np.vstack([dataset[i]['sentence2_binary_parse_index_sequence'] for i in indices])\n",
    "            labels = [dataset[i]['label'] for i in indices]\n",
    "            return premise_vectors, hypothesis_vectors, labels\n",
    "        \n",
    "        self.sess = tf.Session()\n",
    "        \n",
    "        self.sess.run(self.init)\n",
    "        print 'Training.'\n",
    "\n",
    "        # Training cycle\n",
    "        for epoch in range(self.training_epochs):\n",
    "            random.shuffle(training_data)\n",
    "            avg_cost = 0.\n",
    "            total_batch = int(len(training_data) / self.batch_size)\n",
    "            \n",
    "            # Loop over all batches in epoch\n",
    "            for i in range(total_batch):\n",
    "                # Assemble a minibatch of the next B examples\n",
    "                minibatch_premise_vectors, minibatch_hypothesis_vectors, minibatch_labels = get_minibatch(\n",
    "                    training_data, self.batch_size * i, self.batch_size * (i + 1))\n",
    "\n",
    "                # Run the optimizer to take a gradient step, and also fetch the value of the \n",
    "                # cost function for logging\n",
    "                _, c = self.sess.run([self.optimizer, self.total_cost], \n",
    "                                     feed_dict={self.premise_x: minibatch_premise_vectors,\n",
    "                                                self.hypothesis_x: minibatch_hypothesis_vectors,\n",
    "                                                self.y: minibatch_labels})\n",
    "                                                                    \n",
    "                # Compute average loss\n",
    "                avg_cost += c / (total_batch * self.batch_size)\n",
    "                                \n",
    "            # Display some statistics about the step\n",
    "            # Evaluating only one batch worth of data -- simplifies implementation slightly\n",
    "            if (epoch+1) % self.display_epoch_freq == 0:\n",
    "                print \"Epoch:\", (epoch+1), \"Cost:\", avg_cost, \\\n",
    "                    \"Dev acc:\", evaluate_classifier(self.classify, dev_data[0:1000]), \\\n",
    "                    \"Train acc:\", evaluate_classifier(self.classify, training_data[0:1000])  \n",
    "    \n",
    "    def classify(self, examples):\n",
    "        # This classifies a list of examples\n",
    "        premise_vectors = np.vstack([example['sentence1_binary_parse_index_sequence'] for example in examples])\n",
    "        hypothesis_vectors = np.vstack([example['sentence2_binary_parse_index_sequence'] for example in examples])\n",
    "        logits = self.sess.run(self.logits, feed_dict={self.premise_x: premise_vectors,\n",
    "                                                       self.hypothesis_x: hypothesis_vectors})\n",
    "        return np.argmax(logits, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Run the model below. Your goal is dev set performance above 70% within the first 25 epochs, though a successful model may reach as high as 77% depending on your random seed and Python version.\n",
    "\n",
    "Since epochs over the half-million example SNLI corpus are slow, you may wish to debug your model using only a small subset of SNLI by passing training_set[:10000] into classifier.train as its first argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training.\n",
      "Epoch: 1 Cost: 0.004292664038 Dev acc: 0.452527743527 Train acc: 0.441943127962\n",
      "Epoch: 2 Cost: 0.00400245104317 Dev acc: 0.542540073983 Train acc: 0.508960573477\n",
      "Epoch: 3 Cost: 0.0037016659873 Dev acc: 0.62392108508 Train acc: 0.610108303249\n",
      "Epoch: 4 Cost: 0.00334766513846 Dev acc: 0.66954377312 Train acc: 0.649144254279\n",
      "Epoch: 5 Cost: 0.00317433163565 Dev acc: 0.695437731196 Train acc: 0.646989374262\n",
      "Epoch: 6 Cost: 0.00304999522646 Dev acc: 0.695437731196 Train acc: 0.650793650794\n",
      "Epoch: 7 Cost: 0.00297487027203 Dev acc: 0.709001233046 Train acc: 0.693333333333\n",
      "Epoch: 8 Cost: 0.00292156697718 Dev acc: 0.712700369914 Train acc: 0.735115431349\n",
      "Epoch: 9 Cost: 0.00288169242589 Dev acc: 0.72009864365 Train acc: 0.707107843137\n",
      "Epoch: 10 Cost: 0.00284617704552 Dev acc: 0.711467324291 Train acc: 0.716049382716\n",
      "Epoch: 11 Cost: 0.00281861427149 Dev acc: 0.734895191122 Train acc: 0.720144752714\n",
      "Epoch: 12 Cost: 0.00279511099145 Dev acc: 0.736128236745 Train acc: 0.736028537455\n",
      "Epoch: 13 Cost: 0.00277049564724 Dev acc: 0.737361282367 Train acc: 0.70687575392\n",
      "Epoch: 14 Cost: 0.00275054931942 Dev acc: 0.739827373613 Train acc: 0.729397293973\n",
      "Epoch: 15 Cost: 0.00273017020373 Dev acc: 0.744759556104 Train acc: 0.75\n",
      "Epoch: 16 Cost: 0.00271185544452 Dev acc: 0.747225647349 Train acc: 0.709677419355\n",
      "Epoch: 17 Cost: 0.00269405305199 Dev acc: 0.737361282367 Train acc: 0.722424242424\n",
      "Epoch: 18 Cost: 0.00267789171605 Dev acc: 0.733662145499 Train acc: 0.730403800475\n",
      "Epoch: 19 Cost: 0.00266168504554 Dev acc: 0.729963008631 Train acc: 0.760580411125\n",
      "Epoch: 20 Cost: 0.00264553743977 Dev acc: 0.731196054254 Train acc: 0.726060606061\n",
      "Epoch: 21 Cost: 0.00263064241065 Dev acc: 0.733662145499 Train acc: 0.749702026222\n",
      "Epoch: 22 Cost: 0.00261523986435 Dev acc: 0.741060419236 Train acc: 0.737094837935\n",
      "Epoch: 23 Cost: 0.00260008037544 Dev acc: 0.741060419236 Train acc: 0.738007380074\n",
      "Epoch: 24 Cost: 0.00258654960022 Dev acc: 0.750924784217 Train acc: 0.764988009592\n",
      "Epoch: 25 Cost: 0.00257459230773 Dev acc: 0.712700369914 Train acc: 0.715840386941\n"
     ]
    }
   ],
   "source": [
    "classifier = RNNEntailmentClassifier(len(word_indices), SEQ_LEN)\n",
    "classifier.train(training_set, dev_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Questions (30%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1:** Focusing only on the performance of your model on the first ten epochs of training, would adding L2 regularization or dropout help your dev set performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** It wouldn't help much, since during the very first few epochs of training we haven't overfit the data, meaning that the train accuracy and dev accuracy are about the same (dev accuracy can even be better) so generalization won't be a primary issue at the very beginning of training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2:** Write a short script to test the model's performance on dev set examples that contain sentences of more than ten words and those that don't contain such sentences. How does length impact performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The performance on dev set examples that contain sentences of more than ten words is 0.688998362852\n",
      "The performance on dev set examples that do not contain such sentences is 0.727332692923\n"
     ]
    }
   ],
   "source": [
    "def evaluate_classifier2(classifier, eval_set):\n",
    "    correct_more_than_10 = 0\n",
    "    correct_less_than_10 = 0\n",
    "    total_more_than_10 = 0\n",
    "    total_less_than_10 = 0\n",
    "    hypotheses = classifier(eval_set)\n",
    "    for i, example in enumerate(eval_set):\n",
    "        hypothesis = hypotheses[i]\n",
    "        if  len(example['sentence1'].split(' ')) > 10 or len(example['sentence2'].split(' ')) > 10:\n",
    "            total_more_than_10 += 1\n",
    "            if hypothesis == example['label']: \n",
    "                correct_more_than_10 += 1\n",
    "        else:\n",
    "            total_less_than_10 += 1\n",
    "            if hypothesis == example['label']: \n",
    "                correct_less_than_10 += 1\n",
    "    return correct_more_than_10 / float(total_more_than_10), correct_less_than_10 / float(total_less_than_10)\n",
    "\n",
    "correct_more_than_10, correct_less_than_10 = evaluate_classifier2(classifier.classify, dev_set)\n",
    "print \"The performance on dev set examples that contain sentences of more than ten words is \" + str(correct_more_than_10)\n",
    "print \"The performance on dev set examples that do not contain such sentences is \" + str(correct_less_than_10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the results, we see that performance on examples with sentences of more than 10 words is less than that with sentences of less than 10 words. So generally, if the length of sentences is more than the sequence length in the model, then we will probably get a lower accuracy than that whose length is less than the sequence length in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3:** The combination layer uses four different types of input feature. If we skipped the combination layer and fed these features into the softmax classifier layer directly, one of these features would become almost entirely uninformative. Which one is it? (Hint: This is true with SNLI, but may not be true with other corpora.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** It's $\\vec{h}_p - \\vec{h}_h$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
