{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW4: RNNLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, you'll implement an LSTM lanugage model.\n",
    "\n",
    "Submit your completed notebook through NYU Classes by 9:30 AM on October 27."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's load the data as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sst_home = './trees'\n",
    "\n",
    "import re\n",
    "import random\n",
    "\n",
    "# Let's do 2-way positive/negative classification instead of 5-way\n",
    "easy_label_map = {0:0, 1:0, 2:None, 3:1, 4:1}\n",
    "\n",
    "def load_sst_data(path):\n",
    "    data = []\n",
    "    with open(path) as f:\n",
    "        for i, line in enumerate(f): \n",
    "            example = {}\n",
    "            \n",
    "            # Strip out the parse information and the phrase labels---we don't need those here\n",
    "            text = re.sub(r'\\s*(\\(\\d)|(\\))\\s*', '', line)\n",
    "            example['text'] = text[1:]\n",
    "            data.append(example)\n",
    "\n",
    "    random.seed(1)\n",
    "    random.shuffle(data)\n",
    "    return data\n",
    "     \n",
    "training_set = load_sst_data(sst_home + '/train.txt')\n",
    "dev_set = load_sst_data(sst_home + '/dev.txt')\n",
    "test_set = load_sst_data(sst_home + '/test.txt')\n",
    "\n",
    "# Note: Unlike with k-nearest neighbors, evaluation here should be fast, and we don't need to\n",
    "# trim down the dev and test sets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll convert the data to index vectors.\n",
    "\n",
    "To simplify your implementation, we'll use a fixed unrolling length of 20. This means that we'll have to expand each sentence into a sequence of 21 word indices. In the conversion process, we'll mark the start of each sentence with a special word symbol `<S>`, mark the end of each sentence (if it occurs within the first 21 words) with a special word symbol `</S>`, mark extra tokens after `</S>` with a special word symbol `<PAD>`, and mark out-of-vocabulary words with `<UNK>`, for unknown. As in the previous assignment, we'll use a very small vocabulary for this assignment, so you'll see `<UNK>` often."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "\n",
    "def sentence_to_padded_index_sequence(datasets):\n",
    "    '''Annotates datasets with feature vectors.'''\n",
    "    \n",
    "    START = \"<S>\"\n",
    "    END = \"</S>\"\n",
    "    END_PADDING = \"<PAD>\"\n",
    "    UNKNOWN = \"<UNK>\"\n",
    "    SEQ_LEN = 21\n",
    "    \n",
    "    # Extract vocabulary\n",
    "    def tokenize(string):\n",
    "        return string.lower().split()\n",
    "    \n",
    "    word_counter = collections.Counter()\n",
    "    for example in datasets[0]:\n",
    "        word_counter.update(tokenize(example['text']))\n",
    "    \n",
    "    vocabulary = set([word for word in word_counter if word_counter[word] > 25])\n",
    "    vocabulary = list(vocabulary)\n",
    "    vocabulary = [START, END, END_PADDING, UNKNOWN] + vocabulary\n",
    "        \n",
    "    word_indices = dict(zip(vocabulary, range(len(vocabulary))))\n",
    "    indices_to_words = {v: k for k, v in word_indices.items()}\n",
    "        \n",
    "    for i, dataset in enumerate(datasets):\n",
    "        for example in dataset:\n",
    "            example['index_sequence'] = np.zeros((SEQ_LEN), dtype=np.int32)\n",
    "            \n",
    "            token_sequence = [START] + tokenize(example['text']) + [END]\n",
    "            \n",
    "            for i in range(SEQ_LEN):\n",
    "                if i < len(token_sequence):\n",
    "                    if token_sequence[i] in word_indices:\n",
    "                        index = word_indices[token_sequence[i]]\n",
    "                    else:\n",
    "                        index = word_indices[UNKNOWN]\n",
    "                else:\n",
    "                    index = word_indices[END_PADDING]\n",
    "                example['index_sequence'][i] = index\n",
    "    return indices_to_words, word_indices\n",
    "    \n",
    "indices_to_words, word_indices = sentence_to_padded_index_sequence([training_set, dev_set, test_set])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'It arrives with an impeccable pedigree , mongrel pep , and almost indecipherable plot complications .', 'index_sequence': array([  0, 380,   3, 332, 571,   3,   3, 173,   3,   3, 173, 401, 434,\n",
      "         3,  79,   3, 414,   1,   2,   2,   2], dtype=int32)}\n",
      "603\n",
      "plot\n"
     ]
    }
   ],
   "source": [
    "print training_set[18]\n",
    "print len(word_indices)\n",
    "print indices_to_words[79]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Implementation (60%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, using the starter code and hyperparameter values provided below, implement an LSTM language model with dropout on the non-recurrent connections. Use the standard form of the LSTM reflected in the slides (without peepholes). You should only have to edit the marked sections of code to build the base LSTM, though implementing dropout properly may require small changes to the main training loop and to brittle_sampler().\n",
    "\n",
    "Don't use any TensorFlow code that is specifically built for RNNs. If a TF function has 'recurrent', 'sequence', 'LSTM', or 'RNN' in its name, you should built it yourself instead of using it. (Your version will likely be much simpler, by the way, since these built in methods are powerful but fairly complex and potentially confusing.)\n",
    "\n",
    "We won't be evaluating our model in the conventional way (perplexity on a held-out test set) for a few reasons: to save time, because we have no baseline to compare against, and because overfitting the training set is a less immediate concern with these models than it was with sentence classifiers. Instead, we'll use the value of the cost function to make sure that the model is converging as expected, and we'll use samples drawn from the model to qualitatively evaluate it.\n",
    "\n",
    "Tips: \n",
    "\n",
    "- You'll need to use `tf.nn.embedding_lookup()`, `tf.nn.sparse_softmax_cross_entropy_with_logits()`, and `tf.split()` at least once each. All three should be easy to Google, though the last homework and the last exercise should show examples of the first two.\n",
    "- As before, you'll want to initialize your trained parameters using something like `tf.random_normal(..., stddev=0.1)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LanguageModel:\n",
    "    def __init__(self, vocab_size, sequence_length):\n",
    "        # Define the hyperparameters\n",
    "        self.learning_rate = 0.3  # Should be about right\n",
    "        self.training_epochs = 250  # How long to train for - chosen to fit within class time\n",
    "        self.display_epoch_freq = 1  # How often to test and print out statistics\n",
    "        self.dim = 32  # The dimension of the hidden state of the RNN\n",
    "        self.embedding_dim = 16  # The dimension of the learned word embeddings\n",
    "        self.batch_size = 256  # Somewhat arbitrary - can be tuned, but often tune for speed, not accuracy\n",
    "        self.vocab_size = vocab_size  # Defined by the file reader above\n",
    "        self.sequence_length = sequence_length  # Defined by the file reader above\n",
    "        self.keep_rate = 0.75  # Used in dropout (at training time only, not at sampling time)\n",
    "        \n",
    "        # embedding matrix\n",
    "        self.E = tf.Variable(tf.random_normal([self.vocab_size, self.embedding_dim], stddev=0.1))\n",
    "        # state\n",
    "        self.W_rnn = tf.Variable(tf.random_normal([self.embedding_dim + self.dim, self.dim], stddev=0.1))\n",
    "        self.b_rnn = tf.Variable(tf.random_normal([self.dim], stddev=0.1))\n",
    "        # forget gate\n",
    "        self.W_f = tf.Variable(tf.random_normal([self.embedding_dim + self.dim, self.dim], stddev=0.1))\n",
    "        self.b_f = tf.Variable(tf.random_normal([self.dim], stddev=0.1))\n",
    "        # input gate\n",
    "        self.W_i = tf.Variable(tf.random_normal([self.embedding_dim + self.dim, self.dim], stddev=0.1))\n",
    "        self.b_i = tf.Variable(tf.random_normal([self.dim], stddev=0.1))\n",
    "        # output gate\n",
    "        self.W_o = tf.Variable(tf.random_normal([self.embedding_dim + self.dim, self.dim], stddev=0.1))\n",
    "        self.b_o = tf.Variable(tf.random_normal([self.dim], stddev=0.1))\n",
    "        # for softmax\n",
    "        self.W_c = tf.Variable(tf.random_normal([self.dim, self.vocab_size], stddev=0.1))\n",
    "        self.b_c = tf.Variable(tf.random_normal([self.vocab_size], stddev=0.1))\n",
    "        \n",
    "        # Define the input placeholder(s).\n",
    "        self.keep_rate_ph = tf.placeholder(tf.float32, [])\n",
    "        self.x = tf.placeholder(tf.int32, [None, self.sequence_length])\n",
    "        self.y = tf.placeholder(tf.int32, [None, self.sequence_length - 1])\n",
    "        # split apart x and y for different timesteps\n",
    "        self.y_slices = tf.split(1, self.sequence_length-1, self.y)\n",
    "        self.x_slices = tf.split(1, self.sequence_length, self.x)\n",
    "        \n",
    "        # Build the rest of the LSTM LM!\n",
    "        # Define one step of the LSTM\n",
    "        def step(x, c_prev, h_prev):\n",
    "            emb = tf.nn.embedding_lookup(self.E, x)\n",
    "            emb = tf.nn.dropout(emb, self.keep_rate_ph)\n",
    "            emb_h_prev = tf.concat(1, [emb, h_prev])\n",
    "            f = tf.nn.sigmoid(tf.matmul(emb_h_prev, self.W_f)  + self.b_f)\n",
    "            i = tf.nn.sigmoid(tf.matmul(emb_h_prev, self.W_i)  + self.b_i)\n",
    "            c = f*c_prev + i*tf.nn.tanh(tf.matmul(emb_h_prev, self.W_rnn) + self.b_rnn)\n",
    "            o = tf.nn.sigmoid(tf.matmul(emb_h_prev, self.W_o)  + self.b_o)\n",
    "            h = o*tf.nn.tanh(c)\n",
    "            return h, c\n",
    "        \n",
    "        self.h_zero = tf.zeros([self.batch_size, self.dim])\n",
    "        self.c_zero = tf.zeros([self.batch_size, self.dim])\n",
    "        \n",
    "        h_prev = self.h_zero\n",
    "        c_prev = self.c_zero\n",
    "            \n",
    "        # Your model should populate the following four python lists.\n",
    "        # self.logits should contain one [batch_size, vocab_size]-shaped TF tensor of logits \n",
    "        #   for each of the 20 steps of the model.\n",
    "        # self.costs should contain one [batch_size]-shaped TF tensor of cross-entropy loss \n",
    "        #   values for each of the 20 steps of the model.\n",
    "        # self.h and c should each start contain one [batch_size, dim]-shaped TF tensor of LSTM\n",
    "        #   activations for each of the 21 *states* of the model -- one tensor of zeros for the \n",
    "        #   starting state followed by one tensor each for the remaining 20 steps.\n",
    "        # Don't rename any of these variables or change their purpose -- they'll be needed by the\n",
    "        # pre-built sampler.\n",
    "        self.logits = []\n",
    "        self.costs = []\n",
    "        self.h = [self.h_zero]\n",
    "        self.c = [self.c_zero]\n",
    "        \n",
    "        for t in range(self.sequence_length-1):\n",
    "            x_t = tf.reshape(self.x_slices[t], [-1])\n",
    "            y_t = tf.reshape(self.y_slices[t], [-1])\n",
    "            h_prev, c_prev = step(x_t, c_prev, h_prev)\n",
    "            h_prev_drop = tf.nn.dropout(h_prev, self.keep_rate_ph)\n",
    "            logit = tf.matmul(h_prev_drop, self.W_c) + self.b_c\n",
    "            self.logits.append(logit)\n",
    "            loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logit, y_t)\n",
    "            self.costs.append(loss)\n",
    "            self.h.append(h_prev)\n",
    "            self.c.append(c_prev)\n",
    "        \n",
    "        # Sum costs for each word in each example, but average cost across examples.\n",
    "        self.costs_tensor = tf.concat(1, [tf.expand_dims(cost, 1) for cost in self.costs])\n",
    "        self.cost_per_example = tf.reduce_sum(self.costs_tensor, 1)\n",
    "        self.total_cost = tf.reduce_mean(self.cost_per_example)\n",
    "            \n",
    "        # This library call performs the main SGD update equation\n",
    "        self.optimizer = tf.train.GradientDescentOptimizer(self.learning_rate).minimize(self.total_cost)\n",
    "        \n",
    "        # Create an operation to fill zero values in for W and b\n",
    "        self.init = tf.initialize_all_variables()\n",
    "        \n",
    "        # Create a placeholder for the session that will be shared between training and evaluation\n",
    "        self.sess = None\n",
    "        \n",
    "    def train(self, training_data):\n",
    "        def get_minibatch(dataset, start_index, end_index):\n",
    "            indices = range(start_index, end_index)\n",
    "            vectors = np.vstack([dataset[i]['index_sequence'] for i in indices])\n",
    "            return vectors\n",
    "        \n",
    "        self.sess = tf.Session()\n",
    "        \n",
    "        self.sess.run(self.init)\n",
    "        print 'Training.'\n",
    "\n",
    "        # Training cycle\n",
    "        for epoch in range(self.training_epochs):\n",
    "            random.shuffle(training_set)\n",
    "            avg_cost = 0.\n",
    "            total_batch = int(len(training_set) / self.batch_size)\n",
    "            \n",
    "            # Loop over all batches in epoch\n",
    "            for i in range(total_batch):\n",
    "                # Assemble a minibatch of the next B examples\n",
    "                minibatch_vectors = get_minibatch(training_set, self.batch_size * i, self.batch_size * (i + 1))\n",
    "\n",
    "                # Run the optimizer to take a gradient step, and also fetch the value of the \n",
    "                # cost function for logging\n",
    "                _, c = self.sess.run([self.optimizer, self.total_cost], \n",
    "                                     feed_dict={self.x: minibatch_vectors, \n",
    "                                                self.keep_rate_ph: self.keep_rate,\n",
    "                                                self.y: minibatch_vectors[:,1:]})\n",
    "                                                                    \n",
    "                # Compute average loss\n",
    "                avg_cost += c / (total_batch * self.batch_size)\n",
    "                \n",
    "            # Display some statistics about the step\n",
    "            if (epoch+1) % self.display_epoch_freq == 0:\n",
    "                print \"Epoch:\", (epoch+1), \"Cost:\", avg_cost, \"Sample:\", self.sample()\n",
    "    \n",
    "    def sample(self):\n",
    "        # This samples a sequence of tokens from the model starting with <S>.\n",
    "        # We only ever run the first timestep of the model, and use an effective batch size of one\n",
    "        # but we leave the model unrolled for multiple steps, and use the full batch size to simplify \n",
    "        # the training code. This slows things down.\n",
    "\n",
    "        def brittle_sampler():\n",
    "            # The main sampling code. Can fail randomly due to rounding errors that yield probibilities\n",
    "            # that don't sum to one.\n",
    "            \n",
    "            word_indices = [0] # 0 here is the \"<S>\" symbol\n",
    "            for i in range(self.sequence_length - 1):\n",
    "                dummy_x = np.zeros((self.batch_size, self.sequence_length))\n",
    "                dummy_x[0][0] = word_indices[-1]\n",
    "                feed_dict = {self.x: dummy_x,\n",
    "                             self.keep_rate_ph: 1.0}\n",
    "                if i > 0:\n",
    "                    feed_dict[self.h_zero] = h\n",
    "                    feed_dict[self.c_zero] = c\n",
    "                    \n",
    "                h, c, logits = self.sess.run([self.h[1], self.c[1], self.logits[0]], \n",
    "                                             feed_dict=feed_dict)  \n",
    "                logits = logits[0, :] # Discard all but first batch entry\n",
    "                exp_logits = np.exp(logits - np.max(logits))\n",
    "                distribution = exp_logits / exp_logits.sum()\n",
    "                sampled_index = np.flatnonzero(np.random.multinomial(1, distribution))[0]\n",
    "                word_indices.append(sampled_index)\n",
    "            words = [indices_to_words[index] for index in word_indices]\n",
    "            return ' '.join(words)\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                sample = brittle_sampler()\n",
    "                return sample\n",
    "            except ValueError as e:  # Retry if we experience a random failure.\n",
    "                pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train it.\n",
    "\n",
    "Once you're confident your model is doing what you want, let it run for the full 250 epochs. This will take some time—likely between five and thirty minutes. If it much longer on a reasonably modern laptop—more than an hour—that suggests serious problems with your implementation. A properly implemented model with dropout should reach an average cost of less than 0.22 quickly, and then slowly improve from there. We train the model for a fairly long time because these small improvements in cost correspond to fairly large improvements in sample quality.\n",
    "\n",
    "Samples from a trained models should have coherent portions, but they will not resemble interpretable English sentences. Here are three examples from a model with a cost value of 0.202:\n",
    "\n",
    "`<S> the good <UNK> and <UNK> and <UNK> <UNK> with predictable and <UNK> , but also does one of -lrb- <UNK>`\n",
    "\n",
    "`<S> <UNK> has <UNK> actors seems done <UNK> would these <UNK> <UNK> to <UNK> <UNK> <UNK> 're <UNK> to mind .`\n",
    "\n",
    "`<S> an action story that was because the <UNK> <UNK> are when <UNK> as ``` <UNK> '' ' it is any`\n",
    "\n",
    "`-lrb-` and `-rrb` are the way that left and right parentheses are represented in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training.\n",
      "Epoch: 1 Cost: 0.310543827938 Sample: <S> contrived <UNK> <UNK> portrait matter <UNK> that is <UNK> engaging summer </S> </S> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Epoch: 2 Cost: 0.265469183524 Sample: <S> <UNK> , time manages of that an sure as , through he is other mood already place it <UNK> full\n",
      "Epoch: 3 Cost: 0.257904000355 Sample: <S> performances <UNK> <UNK> <UNK> <UNK> out more a <UNK> by of is <UNK> one <UNK> <UNK> <UNK> done ca too\n",
      "Epoch: 4 Cost: 0.252665686788 Sample: <S> a shows be in have love while this one to his <UNK> characters film the <UNK> -lrb- the <UNK> with\n",
      "Epoch: 5 Cost: 0.247719584541 Sample: <S> if american a does but is <UNK> <UNK> that <UNK> genre pleasure and this no , or to <UNK> in\n",
      "Epoch: 6 Cost: 0.244253746939 Sample: <S> the of movie is <UNK> by <UNK> at <UNK> in tale anything <UNK> <UNK> <UNK> social the him <UNK> up\n",
      "Epoch: 7 Cost: 0.241856727636 Sample: <S> as the he , this <UNK> and it <UNK> <UNK> is the still <UNK> not take , end 's rich\n",
      "Epoch: 8 Cost: 0.238957871542 Sample: <S> a <UNK> of but heart too <UNK> <UNK> really -- <UNK> something <UNK> but this was <UNK> : melodrama <UNK>\n",
      "Epoch: 9 Cost: 0.236940764568 Sample: <S> formula moment <UNK> , <UNK> <UNK> up 's -rrb- <UNK> ' is <UNK> crime <UNK> and <UNK> are <UNK> line\n",
      "Epoch: 10 Cost: 0.235661689531 Sample: <S> cold look <UNK> <UNK> high brilliant entertainment that certainly he -- <UNK> by <UNK> -- <UNK> was even one ,\n",
      "Epoch: 11 Cost: 0.233516431668 Sample: <S> <UNK> woman the <UNK> , but it 's year that it are classic , like the <UNK> 's . </S>\n",
      "Epoch: 12 Cost: 0.232782120054 Sample: <S> nothing here good <UNK> , the shot ii <UNK> and a <UNK> and <UNK> for <UNK> <UNK> and work .\n",
      "Epoch: 13 Cost: 0.230643151384 Sample: <S> <UNK> art of 're <UNK> to <UNK> on <UNK> feels takes the <UNK> at <UNK> <UNK> , and <UNK> <UNK>\n",
      "Epoch: 14 Cost: 0.230015230901 Sample: <S> gets not most <UNK> on but a little <UNK> , <UNK> of <UNK> <UNK> <UNK> , <UNK> story that <UNK>\n",
      "Epoch: 15 Cost: 0.229055667917 Sample: <S> the <UNK> film a many as turns lacks <UNK> , half a <UNK> and see the family perfect with <UNK>\n",
      "Epoch: 16 Cost: 0.227755966512 Sample: <S> have the <UNK> <UNK> with <UNK> <UNK> and <UNK> of the whole <UNK> for <UNK> with the <UNK> flick <UNK>\n",
      "Epoch: 17 Cost: 0.227092094042 Sample: <S> your <UNK> to , and boring this <UNK> film with funny action surprisingly <UNK> out and watching a project sure\n",
      "Epoch: 18 Cost: 0.226451904936 Sample: <S> little is like <UNK> for <UNK> <UNK> <UNK> a <UNK> in all by <UNK> , characters on <UNK> its great\n",
      "Epoch: 19 Cost: 0.225627932585 Sample: <S> <UNK> as go the <UNK> <UNK> <UNK> because <UNK> <UNK> us energy that <UNK> rather <UNK> , but a <UNK>\n",
      "Epoch: 20 Cost: 0.224905159889 Sample: <S> contrived 's director , political cast of the <UNK> is times at the star interest , ways out like a\n",
      "Epoch: 21 Cost: 0.223951996276 Sample: <S> the <UNK> like <UNK> <UNK> , <UNK> <UNK> <UNK> <UNK> , fascinating <UNK> the <UNK> with <UNK> and a <UNK>\n",
      "Epoch: 22 Cost: 0.223501548171 Sample: <S> some is a ways <UNK> for it 's <UNK> <UNK> <UNK> about the story , although the production <UNK> <UNK>\n",
      "Epoch: 23 Cost: 0.222841566259 Sample: <S> the fine is <UNK> <UNK> <UNK> does his <UNK> <UNK> from <UNK> for a difficult comedy fine to <UNK> and\n",
      "Epoch: 24 Cost: 0.222538627458 Sample: <S> is one , , <UNK> and the film at it does n't teen beautiful that it <UNK> its and it\n",
      "Epoch: 25 Cost: 0.222425134345 Sample: <S> <UNK> if some first head is guys <UNK> , but there as a look 's <UNK> has jokes never enough\n",
      "Epoch: 26 Cost: 0.221985376694 Sample: <S> the soap filmmakers not no <UNK> , the best <UNK> <UNK> ' <UNK> from a <UNK> <UNK> is the <UNK>\n",
      "Epoch: 27 Cost: 0.221087275581 Sample: <S> a <UNK> <UNK> of <UNK> bland with <UNK> and <UNK> 's <UNK> , but a whole <UNK> from <UNK> and\n",
      "Epoch: 28 Cost: 0.220738938812 Sample: <S> family may now a <UNK> , i 'll , <UNK> <UNK> and it 's <UNK> and the moving of a\n",
      "Epoch: 29 Cost: 0.220476370418 Sample: <S> ... though its <UNK> <UNK> for entertainment by the <UNK> who -rrb- from a <UNK> and his <UNK> <UNK> <UNK>\n",
      "Epoch: 30 Cost: 0.21995943288 Sample: <S> <UNK> <UNK> and <UNK> , <UNK> filmmaker <UNK> of this <UNK> the <UNK> in always <UNK> , <UNK> , in\n",
      "Epoch: 31 Cost: 0.21974007695 Sample: <S> <UNK> <UNK> that <UNK> : going home that <UNK> to not sense and <UNK> <UNK> but without the <UNK> in\n",
      "Epoch: 32 Cost: 0.219366192818 Sample: <S> <UNK> <UNK> and live and <UNK> <UNK> and <UNK> of least the <UNK> with and <UNK> to <UNK> to <UNK>\n",
      "Epoch: 33 Cost: 0.219096880067 Sample: <S> i have a <UNK> <UNK> to <UNK> the movie things too <UNK> that <UNK> life no likely the <UNK> full\n",
      "Epoch: 34 Cost: 0.218813462239 Sample: <S> with take it 's <UNK> to a way that filmmaking it this more <UNK> to <UNK> the two <UNK> ,\n",
      "Epoch: 35 Cost: 0.218200256879 Sample: <S> <UNK> , and the quite the <UNK> <UNK> getting where `` all one of an <UNK> <UNK> '' . </S>\n",
      "Epoch: 36 Cost: 0.218458657915 Sample: <S> it 's you feel like in as least , if all it is <UNK> as <UNK> of <UNK> or <UNK>\n",
      "Epoch: 37 Cost: 0.218041210915 Sample: <S> all yet not all a to his engaging the kind of political fresh , and life is <UNK> , else\n",
      "Epoch: 38 Cost: 0.217860775915 Sample: <S> <UNK> <UNK> is all the <UNK> <UNK> of the fairly comic emotionally <UNK> , which a <UNK> of a <UNK>\n",
      "Epoch: 39 Cost: 0.217698798487 Sample: <S> a <UNK> <UNK> to <UNK> special than as <UNK> , it is the <UNK> <UNK> who <UNK> <UNK> to a\n",
      "Epoch: 40 Cost: 0.217611085736 Sample: <S> an <UNK> <UNK> of being <UNK> very the <UNK> of <UNK> <UNK> in overall , <UNK> and least may made\n",
      "Epoch: 41 Cost: 0.217458072485 Sample: <S> <UNK> manages of <UNK> the <UNK> why <UNK> like the <UNK> of <UNK> the premise of your <UNK> and <UNK>\n",
      "Epoch: 42 Cost: 0.216989355557 Sample: <S> an <UNK> <UNK> <UNK> drama ... from <UNK> <UNK> , <UNK> , and will <UNK> its level , <UNK> and\n",
      "Epoch: 43 Cost: 0.216523546613 Sample: <S> <UNK> writing director even <UNK> of very <UNK> sequel is n't to have our sense that pretentious are <UNK> and\n",
      "Epoch: 44 Cost: 0.216455190471 Sample: <S> the take take all <UNK> of the <UNK> and <UNK> <UNK> genre , also <UNK> under a <UNK> and funny\n",
      "Epoch: 45 Cost: 0.216304304473 Sample: <S> the characters in a <UNK> <UNK> on the <UNK> , <UNK> , despite a better <UNK> at only <UNK> rather\n",
      "Epoch: 46 Cost: 0.216169831879 Sample: <S> a <UNK> <UNK> movie , <UNK> <UNK> <UNK> more <UNK> by too , <UNK> so <UNK> <UNK> <UNK> a guys\n",
      "Epoch: 47 Cost: 0.216025408019 Sample: <S> <UNK> <UNK> come , <UNK> but <UNK> <UNK> <UNK> <UNK> by side , <UNK> , storytelling and little <UNK> <UNK>\n",
      "Epoch: 48 Cost: 0.21568808682 Sample: <S> although the <UNK> <UNK> adventure human <UNK> <UNK> face to <UNK> us to <UNK> a <UNK> , for a <UNK>\n",
      "Epoch: 49 Cost: 0.215646422271 Sample: <S> <UNK> the film <UNK> ultimately <UNK> , <UNK> <UNK> and <UNK> without the very turn out makes <UNK> that out\n",
      "Epoch: 50 Cost: 0.215387805845 Sample: <S> <UNK> writing are a <UNK> <UNK> of <UNK> , <UNK> and <UNK> , but this <UNK> <UNK> <UNK> or <UNK>\n",
      "Epoch: 51 Cost: 0.215284115437 Sample: <S> which it 's <UNK> like a <UNK> <UNK> of a <UNK> <UNK> or <UNK> and <UNK> <UNK> for the women\n",
      "Epoch: 52 Cost: 0.215258326946 Sample: <S> overall <UNK> <UNK> and <UNK> actors to <UNK> , <UNK> <UNK> yet <UNK> <UNK> : has again <UNK> <UNK> a\n",
      "Epoch: 53 Cost: 0.214985108285 Sample: <S> film 's <UNK> in the <UNK> political human over the or <UNK> , <UNK> engaging <UNK> and <UNK> , and\n",
      "Epoch: 54 Cost: 0.214767484954 Sample: <S> here has <UNK> by <UNK> <UNK> - although the <UNK> look in enough to an <UNK> , along <UNK> .\n",
      "Epoch: 55 Cost: 0.214848738728 Sample: <S> ultimately that you have almost <UNK> for this year but how <UNK> the <UNK> most <UNK> music and the <UNK>\n",
      "Epoch: 56 Cost: 0.214372182886 Sample: <S> it should have to <UNK> any make the <UNK> of love in a show <UNK> : the <UNK> <UNK> down\n",
      "Epoch: 57 Cost: 0.214574838678 Sample: <S> the show off the head <UNK> , this film not all its <UNK> , with the family <UNK> of its\n",
      "Epoch: 58 Cost: 0.214272515792 Sample: <S> <UNK> <UNK> at <UNK> <UNK> that 's <UNK> , then the <UNK> <UNK> and <UNK> , on the women .\n",
      "Epoch: 59 Cost: 0.213961457213 Sample: <S> as <UNK> to `` <UNK> ' <UNK> book is not a <UNK> <UNK> and writing ' <UNK> , with <UNK>\n",
      "Epoch: 60 Cost: 0.214057281162 Sample: <S> `` <UNK> '' 's <UNK> might <UNK> after <UNK> that could get be heart as as <UNK> in film in\n",
      "Epoch: 61 Cost: 0.213917368741 Sample: <S> he is might be <UNK> ... it could have to be him the performances but <UNK> <UNK> a kind of\n",
      "Epoch: 62 Cost: 0.213465739832 Sample: <S> it 's <UNK> to be the worst year , but this looks <UNK> , it 's up about contrived .\n",
      "Epoch: 63 Cost: 0.213886083527 Sample: <S> one <UNK> at to <UNK> as for there <UNK> up a <UNK> of <UNK> , probably may not all ,\n",
      "Epoch: 64 Cost: 0.213672872294 Sample: <S> <UNK> you truly care on bad as what comes more many as <UNK> <UNK> <UNK> is , which is <UNK>\n",
      "Epoch: 65 Cost: 0.21344673453 Sample: <S> looks is the <UNK> of the time , <UNK> <UNK> <UNK> <UNK> and a world for as <UNK> it .\n",
      "Epoch: 66 Cost: 0.213288768674 Sample: <S> an <UNK> <UNK> and <UNK> <UNK> i to make by some <UNK> <UNK> <UNK> in the <UNK> <UNK> movie in\n",
      "Epoch: 67 Cost: 0.213103779338 Sample: <S> as once <UNK> 's original <UNK> to <UNK> ever too be the film <UNK> out him , star is a\n",
      "Epoch: 68 Cost: 0.213243758588 Sample: <S> <UNK> is the best movie with his <UNK> <UNK> as new as the boring is n't <UNK> on the camera\n",
      "Epoch: 69 Cost: 0.212885872884 Sample: <S> <UNK> <UNK> 's the thoroughly <UNK> <UNK> <UNK> been funny , <UNK> <UNK> <UNK> , <UNK> <UNK> , <UNK> performances\n",
      "Epoch: 70 Cost: 0.213037707137 Sample: <S> <UNK> <UNK> <UNK> <UNK> is <UNK> and an two <UNK> <UNK> ... the <UNK> of the <UNK> of the <UNK>\n",
      "Epoch: 71 Cost: 0.212714191639 Sample: <S> <UNK> , <UNK> old satisfying <UNK> into <UNK> as <UNK> feature <UNK> 's <UNK> <UNK> of <UNK> <UNK> as the\n",
      "Epoch: 72 Cost: 0.212857339418 Sample: <S> like be <UNK> '' ... this half about the script will have ever been the <UNK> , but <UNK> my\n",
      "Epoch: 73 Cost: 0.212551419031 Sample: <S> , <UNK> to <UNK> its tone to keep the <UNK> of <UNK> what <UNK> as <UNK> , <UNK> <UNK> and\n",
      "Epoch: 74 Cost: 0.212771505569 Sample: <S> a <UNK> <UNK> <UNK> was <UNK> about <UNK> , at another or the <UNK> with work , the movie is\n",
      "Epoch: 75 Cost: 0.212332031944 Sample: <S> the <UNK> <UNK> , <UNK> <UNK> 's some <UNK> , never nearly more have <UNK> <UNK> that <UNK> 's <UNK>\n",
      "Epoch: 76 Cost: 0.212085988485 Sample: <S> a <UNK> performance that 's you <UNK> to be <UNK> at <UNK> before , but those <UNK> at the <UNK>\n",
      "Epoch: 77 Cost: 0.212360266935 Sample: <S> if you take 's <UNK> , the characters <UNK> <UNK> <UNK> the <UNK> <UNK> -lrb- <UNK> -rrb- everything i !\n",
      "Epoch: 78 Cost: 0.212157341115 Sample: <S> what 's <UNK> of this <UNK> and book <UNK> <UNK> ... ... as you really <UNK> out since best play\n",
      "Epoch: 79 Cost: 0.212266213515 Sample: <S> <UNK> <UNK> , <UNK> and , <UNK> <UNK> from the <UNK> <UNK> of <UNK> 's visual the films to only\n",
      "Epoch: 80 Cost: 0.21195293692 Sample: <S> -lrb- charm -rrb- <UNK> is a <UNK> <UNK> that , while it 's <UNK> seeing hilarious life and <UNK> <UNK>\n",
      "Epoch: 81 Cost: 0.212063837684 Sample: <S> it 's <UNK> of <UNK> actors less than <UNK> 's <UNK> , <UNK> <UNK> , romantic <UNK> <UNK> sad .\n",
      "Epoch: 82 Cost: 0.212209977887 Sample: <S> the film is a <UNK> good play that would been <UNK> for its as a <UNK> performance in <UNK> by\n",
      "Epoch: 83 Cost: 0.211812792854 Sample: <S> a <UNK> <UNK> beyond the <UNK> <UNK> , <UNK> war to be nor even one with the camera <UNK> where\n",
      "Epoch: 84 Cost: 0.212040571552 Sample: <S> a <UNK> mr. <UNK> <UNK> -- making a title script that <UNK> <UNK> <UNK> and not <UNK> <UNK> and a\n",
      "Epoch: 85 Cost: 0.21180569945 Sample: <S> the <UNK> <UNK> with <UNK> 's drama with the this men <UNK> <UNK> give so be <UNK> and it must\n",
      "Epoch: 86 Cost: 0.211565727086 Sample: <S> if as <UNK> <UNK> any of it she comes to care with a very <UNK> too <UNK> to work far\n",
      "Epoch: 87 Cost: 0.211652520931 Sample: <S> if the first once had not a great <UNK> falls , the film is much after most of <UNK> ,\n",
      "Epoch: 88 Cost: 0.211474924828 Sample: <S> an engaging of ... charm that 'll be a <UNK> <UNK> <UNK> <UNK> and <UNK> , it is nothing <UNK>\n",
      "Epoch: 89 Cost: 0.211451520071 Sample: <S> both light by with the plot 's <UNK> 's an <UNK> as the <UNK> <UNK> <UNK> <UNK> like the <UNK>\n",
      "Epoch: 90 Cost: 0.211354814696 Sample: <S> if in about <UNK> and <UNK> , solid <UNK> in its violence and most memorable <UNK> off the movie or\n",
      "Epoch: 91 Cost: 0.211083009839 Sample: <S> like any <UNK> of <UNK> , <UNK> <UNK> lacks an <UNK> of worst of and <UNK> and , <UNK> events\n",
      "Epoch: 92 Cost: 0.211105567939 Sample: <S> with most <UNK> and <UNK> <UNK> that does you could make us <UNK> , nor it still -lrb- a <UNK>\n",
      "Epoch: 93 Cost: 0.211113364859 Sample: <S> but one of the <UNK> of his <UNK> set has <UNK> from his <UNK> , <UNK> , <UNK> <UNK> ,\n",
      "Epoch: 94 Cost: 0.211044874155 Sample: <S> an idea of <UNK> <UNK> <UNK> , moving of her <UNK> <UNK> to <UNK> of <UNK> an turn , when\n",
      "Epoch: 95 Cost: 0.210998289513 Sample: <S> just a quirky <UNK> that ... <UNK> into a <UNK> <UNK> and the <UNK> of a <UNK> <UNK> , the\n",
      "Epoch: 96 Cost: 0.210869116765 Sample: <S> an <UNK> movie , <UNK> <UNK> <UNK> into the interest in the <UNK> <UNK> <UNK> <UNK> in his <UNK> love\n",
      "Epoch: 97 Cost: 0.210820108201 Sample: <S> a <UNK> sad <UNK> , full of those <UNK> reason to <UNK> to <UNK> to be end , `` <UNK>\n",
      "Epoch: 98 Cost: 0.211075786388 Sample: <S> the <UNK> may be <UNK> for anyone it other on the time for mostly before <UNK> and <UNK> <UNK> </S>\n",
      "Epoch: 99 Cost: 0.210872315548 Sample: <S> to <UNK> the comedy <UNK> for the message <UNK> in her <UNK> for <UNK> <UNK> and either will when rare\n",
      "Epoch: 100 Cost: 0.210923998193 Sample: <S> this <UNK> <UNK> is <UNK> -- making deeply women that <UNK> , is rather acting but <UNK> <UNK> in <UNK>\n",
      "Epoch: 101 Cost: 0.210844650413 Sample: <S> it 's seems <UNK> away with the first <UNK> cliches of plot , dramatic , who have the last <UNK>\n",
      "Epoch: 102 Cost: 0.210506951267 Sample: <S> while <UNK> <UNK> directed to the film 's <UNK> 's <UNK> their own lives , and offers <UNK> <UNK> -\n",
      "Epoch: 103 Cost: 0.210386792367 Sample: <S> although too <UNK> enough <UNK> , <UNK> to keep their subject <UNK> remains some flat <UNK> <UNK> by we <UNK>\n",
      "Epoch: 104 Cost: 0.210731325728 Sample: <S> there is <UNK> <UNK> , and <UNK> and <UNK> <UNK> is so have , <UNK> is solid <UNK> , almost\n",
      "Epoch: 105 Cost: 0.210789447933 Sample: <S> if it 's more to <UNK> , the <UNK> is all the same <UNK> that are <UNK> to be <UNK>\n",
      "Epoch: 106 Cost: 0.210456593019 Sample: <S> a <UNK> <UNK> make where a <UNK> <UNK> to make at every perfect <UNK> effort <UNK> , and you 've\n",
      "Epoch: 107 Cost: 0.21033652488 Sample: <S> <UNK> <UNK> and <UNK> <UNK> jokes , this film is <UNK> that it as the most <UNK> rather <UNK> <UNK>\n",
      "Epoch: 108 Cost: 0.21060688884 Sample: <S> <UNK> and two <UNK> <UNK> heart is a film , <UNK> <UNK> <UNK> over us <UNK> <UNK> of <UNK> <UNK>\n",
      "Epoch: 109 Cost: 0.210405038162 Sample: <S> the script is no performances <UNK> did n't set a <UNK> , but <UNK> almost easy a film so a\n",
      "Epoch: 110 Cost: 0.210213576754 Sample: <S> a <UNK> <UNK> that delivers the written <UNK> <UNK> humor into the <UNK> of one of the <UNK> narrative ,\n",
      "Epoch: 111 Cost: 0.210259512518 Sample: <S> play entertaining <UNK> off looking and <UNK> with <UNK> with the message to to <UNK> to see , clever they\n",
      "Epoch: 112 Cost: 0.210138459097 Sample: <S> it 's hard and a entertaining <UNK> <UNK> of the <UNK> in <UNK> <UNK> , <UNK> <UNK> to this <UNK>\n",
      "Epoch: 113 Cost: 0.210151038387 Sample: <S> <UNK> <UNK> could be <UNK> in <UNK> and <UNK> your <UNK> that <UNK> here it are so much of ,\n",
      "Epoch: 114 Cost: 0.210068136454 Sample: <S> <UNK> <UNK> sequences when 's <UNK> my what 's <UNK> up of <UNK> to the <UNK> director people but <UNK>\n",
      "Epoch: 115 Cost: 0.21006409282 Sample: <S> <UNK> , <UNK> all with along the fact , the <UNK> <UNK> by <UNK> <UNK> -rrb- the feeling whose performances\n",
      "Epoch: 116 Cost: 0.210089464982 Sample: <S> the <UNK> <UNK> moment all <UNK> the <UNK> , <UNK> that is worth the <UNK> -lrb- <UNK> gets this <UNK>\n",
      "Epoch: 117 Cost: 0.209913898147 Sample: <S> its <UNK> thing <UNK> through us black entertainment in <UNK> at the familiar other than <UNK> without <UNK> a <UNK>\n",
      "Epoch: 118 Cost: 0.210231699727 Sample: <S> <UNK> 's children is <UNK> <UNK> <UNK> into <UNK> , <UNK> , a <UNK> , <UNK> , <UNK> <UNK> and\n",
      "Epoch: 119 Cost: 0.210142411517 Sample: <S> it 's hard and funny to <UNK> them , compelling home and little <UNK> <UNK> <UNK> well a tv time\n",
      "Epoch: 120 Cost: 0.209832705332 Sample: <S> <UNK> and way as for <UNK> the high <UNK> , they are one of the <UNK> narrative piece of the\n",
      "Epoch: 121 Cost: 0.209614337845 Sample: <S> of the actors <UNK> in <UNK> in the <UNK> video <UNK> <UNK> here makes just of the <UNK> <UNK> in\n",
      "Epoch: 122 Cost: 0.209734511195 Sample: <S> `` the <UNK> american <UNK> '' <UNK> , it 's <UNK> out for <UNK> , and the most become all\n",
      "Epoch: 123 Cost: 0.20979498101 Sample: <S> ... the end are the screen , but do n't can <UNK> <UNK> <UNK> <UNK> to the film <UNK> ,\n",
      "Epoch: 124 Cost: 0.209974364349 Sample: <S> too the <UNK> sad , <UNK> and , but that <UNK> , they still 're not a charm of <UNK>\n",
      "Epoch: 125 Cost: 0.20957601838 Sample: <S> <UNK> <UNK> between tale -- this rare <UNK> <UNK> of its <UNK> and <UNK> and a moving as it also\n",
      "Epoch: 126 Cost: 0.209759510828 Sample: <S> <UNK> is n't a hours at war and as a <UNK> , <UNK> <UNK> is <UNK> about your <UNK> .\n",
      "Epoch: 127 Cost: 0.209235388673 Sample: <S> the film is wonderful of <UNK> as a <UNK> <UNK> <UNK> <UNK> ; the director if we <UNK> here .\n",
      "Epoch: 128 Cost: 0.209619049773 Sample: <S> michael young <UNK> like a <UNK> <UNK> <UNK> about <UNK> <UNK> on the <UNK> <UNK> of plot and great smart\n",
      "Epoch: 129 Cost: 0.209375170144 Sample: <S> the <UNK> , on all all plot , enjoyable , the <UNK> , <UNK> <UNK> <UNK> performances production , <UNK>\n",
      "Epoch: 130 Cost: 0.209404424736 Sample: <S> a full power of <UNK> , often <UNK> <UNK> <UNK> of <UNK> its <UNK> <UNK> , and no <UNK> and\n",
      "Epoch: 131 Cost: 0.209248623613 Sample: <S> in the young family <UNK> <UNK> and point , were enough to <UNK> at the <UNK> family man 's <UNK>\n",
      "Epoch: 132 Cost: 0.209430648973 Sample: <S> i 'd just ever come to more to <UNK> their <UNK> <UNK> minutes of the <UNK> world by the <UNK>\n",
      "Epoch: 133 Cost: 0.209285750985 Sample: <S> not <UNK> to find hard , it was a <UNK> <UNK> that <UNK> as the love between great film .\n",
      "Epoch: 134 Cost: 0.209130102938 Sample: <S> it 's , the <UNK> direction <UNK> for no cold stuff to <UNK> <UNK> up , <UNK> under the <UNK>\n",
      "Epoch: 135 Cost: 0.209337565483 Sample: <S> <UNK> <UNK> 's <UNK> performance out of the <UNK> video movie a <UNK> <UNK> , and the <UNK> <UNK> .\n",
      "Epoch: 136 Cost: 0.209002678593 Sample: <S> it was something to <UNK> <UNK> her <UNK> with <UNK> , <UNK> and <UNK> <UNK> on <UNK> to be also\n",
      "Epoch: 137 Cost: 0.209326875932 Sample: <S> <UNK> and <UNK> <UNK> , the film is the <UNK> <UNK> to be very <UNK> <UNK> to a <UNK> <UNK>\n",
      "Epoch: 138 Cost: 0.209150073655 Sample: <S> good series to make the <UNK> of that <UNK> interest that she <UNK> the production and <UNK> <UNK> are be\n",
      "Epoch: 139 Cost: 0.209256043488 Sample: <S> there 's hard to <UNK> the subject <UNK> of french emotional <UNK> <UNK> <UNK> <UNK> its <UNK> and <UNK> and\n",
      "Epoch: 140 Cost: 0.209348055449 Sample: <S> it 's like <UNK> to <UNK> too likely , <UNK> , an <UNK> very <UNK> <UNK> that will really <UNK>\n",
      "Epoch: 141 Cost: 0.209122321371 Sample: <S> <UNK> <UNK> <UNK> <UNK> of a great too feeling watching that they 're <UNK> , <UNK> <UNK> , a <UNK>\n",
      "Epoch: 142 Cost: 0.209014908834 Sample: <S> <UNK> <UNK> itself like a lot of <UNK> to an <UNK> comedy , the is too <UNK> and <UNK> in\n",
      "Epoch: 143 Cost: 0.209056378314 Sample: <S> one of the dark films about <UNK> recent <UNK> , or <UNK> the worst way to the story <UNK> <UNK>\n",
      "Epoch: 144 Cost: 0.208980080305 Sample: <S> the filmmakers thing scenes if no <UNK> of the characters <UNK> <UNK> a real premise , i have n't more\n",
      "Epoch: 145 Cost: 0.209180902803 Sample: <S> a <UNK> and movie about a heart , is a <UNK> `` going and `` strong movie and <UNK> <UNK>\n",
      "Epoch: 146 Cost: 0.20900147476 Sample: <S> very <UNK> <UNK> on the <UNK> ... <UNK> , even <UNK> how <UNK> <UNK> in the <UNK> , and believe\n",
      "Epoch: 147 Cost: 0.208791449666 Sample: <S> if <UNK> to be the best <UNK> with viewer , hollywood so <UNK> this thriller by <UNK> suspense and <UNK>\n",
      "Epoch: 148 Cost: 0.208872858774 Sample: <S> its <UNK> <UNK> with <UNK> and <UNK> , the filmmakers <UNK> has be the <UNK> that this <UNK> . </S>\n",
      "Epoch: 149 Cost: 0.20895888408 Sample: <S> a <UNK> and <UNK> <UNK> comedy is a difficult of <UNK> <UNK> and something that <UNK> this director would have\n",
      "Epoch: 150 Cost: 0.208818147128 Sample: <S> ... the wonderful and <UNK> <UNK> and <UNK> dialogue that has easily <UNK> from <UNK> 's <UNK> of this tragedy\n",
      "Epoch: 151 Cost: 0.208903525363 Sample: <S> it 's not all <UNK> <UNK> -- <UNK> and <UNK> on an <UNK> <UNK> , <UNK> drama that into a\n",
      "Epoch: 152 Cost: 0.208771014755 Sample: <S> a <UNK> <UNK> portrait of <UNK> and <UNK> -- for the <UNK> <UNK> , <UNK> and dark their <UNK> <UNK>\n",
      "Epoch: 153 Cost: 0.208564998074 Sample: <S> <UNK> <UNK> and <UNK> <UNK> <UNK> of <UNK> but <UNK> n't <UNK> together <UNK> , and it comes <UNK> to\n",
      "Epoch: 154 Cost: 0.208663414374 Sample: <S> ... the movie , much of all <UNK> <UNK> <UNK> <UNK> gags of just a new <UNK> full tale for\n",
      "Epoch: 155 Cost: 0.208780602524 Sample: <S> -lrb- `` <UNK> -rrb- remains a <UNK> of <UNK> -lrb- <UNK> with <UNK> -rrb- <UNK> of a <UNK> <UNK> <UNK>\n",
      "Epoch: 156 Cost: 0.208738531127 Sample: <S> the film are on an <UNK> of a <UNK> , mostly <UNK> and an much impossible intelligence out action and\n",
      "Epoch: 157 Cost: 0.20873971735 Sample: <S> but the kind of <UNK> <UNK> the <UNK> special for the <UNK> it the <UNK> of its <UNK> end ,\n",
      "Epoch: 158 Cost: 0.208884301511 Sample: <S> imagine a <UNK> <UNK> , to the <UNK> with the <UNK> and <UNK> <UNK> himself against <UNK> <UNK> <UNK> on\n",
      "Epoch: 159 Cost: 0.208639119159 Sample: <S> an film that the dark <UNK> the horror <UNK> before <UNK> <UNK> <UNK> out the day <UNK> ... but this\n",
      "Epoch: 160 Cost: 0.208640995802 Sample: <S> just the strange <UNK> of <UNK> <UNK> minutes again , too <UNK> <UNK> into the <UNK> for <UNK> <UNK> up\n",
      "Epoch: 161 Cost: 0.208503804424 Sample: <S> one who could be <UNK> to <UNK> for <UNK> , an <UNK> <UNK> to the <UNK> movie 's <UNK> with\n",
      "Epoch: 162 Cost: 0.208573649779 Sample: <S> and the <UNK> <UNK> <UNK> <UNK> <UNK> is so <UNK> fun , but they can enjoy as the movie .\n",
      "Epoch: 163 Cost: 0.208431241187 Sample: <S> as <UNK> 's too intriguing <UNK> to do <UNK> to see to be melodrama and all of <UNK> <UNK> .\n",
      "Epoch: 164 Cost: 0.208517985814 Sample: <S> a film <UNK> to a rich between <UNK> , and it 's an <UNK> 's <UNK> of violence with the\n",
      "Epoch: 165 Cost: 0.208474736322 Sample: <S> <UNK> <UNK> <UNK> <UNK> minutes else so matter and <UNK> by a <UNK> <UNK> and sometimes shot it . </S>\n",
      "Epoch: 166 Cost: 0.208573950963 Sample: <S> at a <UNK> <UNK> <UNK> that is <UNK> and <UNK> when you part if you 're <UNK> in while the\n",
      "Epoch: 167 Cost: 0.208609864116 Sample: <S> is a <UNK> <UNK> of <UNK> , and there 's <UNK> as every these <UNK> <UNK> <UNK> <UNK> here is\n",
      "Epoch: 168 Cost: 0.208442411188 Sample: <S> ... <UNK> that it <UNK> times off -- <UNK> <UNK> are well to <UNK> -- and this <UNK> <UNK> <UNK>\n",
      "Epoch: 169 Cost: 0.208578299392 Sample: <S> <UNK> <UNK> , the <UNK> stuff is made <UNK> than <UNK> <UNK> ; <UNK> manages <UNK> to an <UNK> day\n",
      "Epoch: 170 Cost: 0.208211312691 Sample: <S> it 's few <UNK> entertaining <UNK> who <UNK> above some of the <UNK> <UNK> one <UNK> at a <UNK> <UNK>\n",
      "Epoch: 171 Cost: 0.208311247555 Sample: <S> the film 's <UNK> <UNK> images is not a much <UNK> who 'll have <UNK> out , is that 's\n",
      "Epoch: 172 Cost: 0.208415818485 Sample: <S> a <UNK> of an solid comedy , big woman 's <UNK> but a crime piece of <UNK> new <UNK> <UNK>\n",
      "Epoch: 173 Cost: 0.20817471273 Sample: <S> -lrb- the director of -rrb- beautiful short , <UNK> <UNK> that 's <UNK> <UNK> -rrb- <UNK> , <UNK> . </S>\n",
      "Epoch: 174 Cost: 0.208186248487 Sample: <S> a <UNK> , <UNK> and <UNK> <UNK> , if there 's <UNK> , rather <UNK> , and does dark <UNK>\n",
      "Epoch: 175 Cost: 0.208126610427 Sample: <S> <UNK> , sure <UNK> , and its <UNK> <UNK> and truly <UNK> kids to <UNK> , just made it also\n",
      "Epoch: 176 Cost: 0.208133155649 Sample: <S> i <UNK> in once that he 's <UNK> then are funny <UNK> <UNK> without <UNK> <UNK> and director women ,\n",
      "Epoch: 177 Cost: 0.208206071095 Sample: <S> the <UNK> , movies with <UNK> <UNK> is impossible off the <UNK> but <UNK> <UNK> as an great takes and\n",
      "Epoch: 178 Cost: 0.208136064085 Sample: <S> <UNK> <UNK> that <UNK> mystery , occasionally <UNK> , <UNK> , nothing scene ... a <UNK> good and plays <UNK>\n",
      "Epoch: 179 Cost: 0.208010642818 Sample: <S> <UNK> , <UNK> , hour , an <UNK> script that <UNK> makes you can look like a <UNK> one 'd\n",
      "Epoch: 180 Cost: 0.208082137234 Sample: <S> it may be anything to see the actors -lrb- its girl <UNK> in for john <UNK> . ' </S> <PAD>\n",
      "Epoch: 181 Cost: 0.20814374902 Sample: <S> <UNK> and the <UNK> are <UNK> on the <UNK> , <UNK> with how <UNK> has become enough for <UNK> .\n",
      "Epoch: 182 Cost: 0.208403959419 Sample: <S> <UNK> <UNK> the <UNK> , the performances <UNK> cast has a intriguing book <UNK> , but a movie for the\n",
      "Epoch: 183 Cost: 0.208225127422 Sample: <S> it 's do his anyone <UNK> <UNK> by given that <UNK> of kind , <UNK> <UNK> <UNK> of <UNK> another\n",
      "Epoch: 184 Cost: 0.208250018232 Sample: <S> <UNK> <UNK> and contrived and our <UNK> of a dramatic <UNK> in the <UNK> <UNK> that goes contrived to <UNK>\n",
      "Epoch: 185 Cost: 0.207962848923 Sample: <S> <UNK> <UNK> <UNK> <UNK> and <UNK> <UNK> by <UNK> and <UNK> to take enough to have <UNK> to be <UNK>\n",
      "Epoch: 186 Cost: 0.208027412042 Sample: <S> a satisfying , <UNK> <UNK> , <UNK> <UNK> , <UNK> <UNK> ... and <UNK> <UNK> the sense of <UNK> and\n",
      "Epoch: 187 Cost: 0.20805129454 Sample: <S> i 'll <UNK> <UNK> , but <UNK> <UNK> very <UNK> <UNK> michael <UNK> ... but <UNK> making not <UNK> for\n",
      "Epoch: 188 Cost: 0.207935412725 Sample: <S> it 's just all better with this engaging look who <UNK> as <UNK> a <UNK> for social most of our\n",
      "Epoch: 189 Cost: 0.208042106845 Sample: <S> a <UNK> , look too strange to <UNK> they have seen with solid way by a few <UNK> . </S>\n",
      "Epoch: 190 Cost: 0.20805753903 Sample: <S> a <UNK> <UNK> comedy that <UNK> <UNK> the <UNK> of a tv one <UNK> <UNK> has a bit of <UNK>\n",
      "Epoch: 191 Cost: 0.207788968177 Sample: <S> if not <UNK> <UNK> to be cold , i 'll <UNK> to find <UNK> over it is going : <UNK>\n",
      "Epoch: 192 Cost: 0.208081217426 Sample: <S> it 's none and your <UNK> plot , in high storytelling , <UNK> <UNK> and <UNK> gets <UNK> animation up\n",
      "Epoch: 193 Cost: 0.207863297426 Sample: <S> <UNK> with michael <UNK> and <UNK> again into the of <UNK> , as <UNK> beyond his <UNK> <UNK> <UNK> beyond\n",
      "Epoch: 194 Cost: 0.207702218583 Sample: <S> <UNK> <UNK> year , the <UNK> , <UNK> <UNK> and <UNK> <UNK> and <UNK> life of for it <UNK> .\n",
      "Epoch: 195 Cost: 0.207783965902 Sample: <S> the plot ends quite <UNK> to make themselves <UNK> , but it 's <UNK> to watch and <UNK> , <UNK>\n",
      "Epoch: 196 Cost: 0.207984333689 Sample: <S> the rich <UNK> and is <UNK> , <UNK> a bit of an <UNK> lot comedy and a few <UNK> <UNK>\n",
      "Epoch: 197 Cost: 0.207932713357 Sample: <S> power comedy is sweet sense of <UNK> <UNK> , in <UNK> of <UNK> , <UNK> and <UNK> in <UNK> there\n",
      "Epoch: 198 Cost: 0.207971714211 Sample: <S> a mr. film <UNK> <UNK> 's <UNK> <UNK> in the <UNK> and <UNK> 's <UNK> enough for <UNK> <UNK> ,\n",
      "Epoch: 199 Cost: 0.207886674639 Sample: <S> the film is becomes <UNK> in a <UNK> <UNK> <UNK> of the <UNK> <UNK> about <UNK> <UNK> 's <UNK> <UNK>\n",
      "Epoch: 200 Cost: 0.207628856554 Sample: <S> <UNK> <UNK> the man is an <UNK> between they to do its <UNK> the <UNK> for <UNK> when a <UNK>\n",
      "Epoch: 201 Cost: 0.207686174548 Sample: <S> a very old <UNK> , one <UNK> like <UNK> <UNK> works of material that would be <UNK> in -lrb- <UNK>\n",
      "Epoch: 202 Cost: 0.207798095815 Sample: <S> <UNK> , <UNK> light and bad , <UNK> <UNK> life and <UNK> does n't be at n't that is <UNK>\n",
      "Epoch: 203 Cost: 0.207901583928 Sample: <S> as she had <UNK> , some <UNK> ... <UNK> <UNK> , <UNK> <UNK> directed and his <UNK> <UNK> and <UNK>\n",
      "Epoch: 204 Cost: 0.207500816746 Sample: <S> <UNK> that comes <UNK> up <UNK> , and the film does the movie still up <UNK> , that the real\n",
      "Epoch: 205 Cost: 0.20762554082 Sample: <S> overall it was one <UNK> in <UNK> with <UNK> <UNK> <UNK> <UNK> people of <UNK> <UNK> , and are funny\n",
      "Epoch: 206 Cost: 0.207700884703 Sample: <S> did fails out a <UNK> -- the theater <UNK> <UNK> is <UNK> <UNK> very <UNK> <UNK> well the drama with\n",
      "Epoch: 207 Cost: 0.207671493743 Sample: <S> while its art <UNK> is <UNK> , and has been <UNK> the <UNK> of the <UNK> <UNK> on <UNK> ,\n",
      "Epoch: 208 Cost: 0.207617710034 Sample: <S> as many other interesting , this is a smart tale that <UNK> a <UNK> rather easy in <UNK> <UNK> of\n",
      "Epoch: 209 Cost: 0.207538983136 Sample: <S> it 's too <UNK> to <UNK> on its <UNK> <UNK> <UNK> the story between <UNK> and a day without the\n",
      "Epoch: 210 Cost: 0.207591704347 Sample: <S> <UNK> and `` , <UNK> ... i have a <UNK> of a <UNK> <UNK> performance <UNK> , premise as your\n",
      "Epoch: 211 Cost: 0.207534420671 Sample: <S> an good <UNK> <UNK> <UNK> on the classic film <UNK> , its <UNK> is so <UNK> in <UNK> <UNK> and\n",
      "Epoch: 212 Cost: 0.207696942217 Sample: <S> this is fascinating to <UNK> us to <UNK> the movie <UNK> to never <UNK> a <UNK> 's that it 's\n",
      "Epoch: 213 Cost: 0.207478215749 Sample: <S> : put out at sex : i say about the work of <UNK> : <UNK> the <UNK> to be <UNK>\n",
      "Epoch: 214 Cost: 0.207688042612 Sample: <S> <UNK> little minutes of a <UNK> <UNK> of <UNK> <UNK> <UNK> in his <UNK> performance that <UNK> it ... <UNK>\n",
      "Epoch: 215 Cost: 0.207506599751 Sample: <S> but what of there is <UNK> in a <UNK> <UNK> of <UNK> <UNK> -- but i <UNK> <UNK> <UNK> a\n",
      "Epoch: 216 Cost: 0.207573646849 Sample: <S> ... boring , not with <UNK> dark , <UNK> it has the <UNK> <UNK> <UNK> -- which there is a\n",
      "Epoch: 217 Cost: 0.207528637666 Sample: <S> the is <UNK> and <UNK> a <UNK> at <UNK> in all , or that <UNK> <UNK> 's <UNK> -- as\n",
      "Epoch: 218 Cost: 0.207525097511 Sample: <S> a <UNK> of <UNK> is a mystery <UNK> debut nearly <UNK> as a clever film in the most <UNK> <UNK>\n",
      "Epoch: 219 Cost: 0.207447396986 Sample: <S> an big <UNK> , <UNK> <UNK> <UNK> , who were all all the worst of sweet they with a <UNK>\n",
      "Epoch: 220 Cost: 0.207611412713 Sample: <S> a enjoyable <UNK> of the <UNK> satisfying <UNK> and <UNK> thriller a movie that <UNK> to the face <UNK> with\n",
      "Epoch: 221 Cost: 0.207602734819 Sample: <S> i 've seen you away got no dialogue and but it should not be <UNK> good by the story <UNK>\n",
      "Epoch: 222 Cost: 0.207331274495 Sample: <S> a <UNK> family drama that 's not pretty <UNK> <UNK> , it <UNK> it in your <UNK> , it 's\n",
      "Epoch: 223 Cost: 0.207530053276 Sample: <S> <UNK> <UNK> a project the director <UNK> for a culture of a <UNK> <UNK> of <UNK> <UNK> who are <UNK>\n",
      "Epoch: 224 Cost: 0.207371858485 Sample: <S> is a bit <UNK> about <UNK> in a great theater flick all the <UNK> <UNK> of <UNK> and the <UNK>\n",
      "Epoch: 225 Cost: 0.207470906052 Sample: <S> an <UNK> <UNK> , <UNK> <UNK> and <UNK> little <UNK> of <UNK> feels be <UNK> to make its <UNK> as\n",
      "Epoch: 226 Cost: 0.207435034441 Sample: <S> you 're <UNK> for <UNK> for <UNK> <UNK> minutes <UNK> on <UNK> both <UNK> that it you do not know\n",
      "Epoch: 227 Cost: 0.207323162393 Sample: <S> for <UNK> to <UNK> the <UNK> of <UNK> , who <UNK> <UNK> as seeing <UNK> with the the way <UNK>\n",
      "Epoch: 228 Cost: 0.207230434725 Sample: <S> it 's an <UNK> <UNK> <UNK> before the thing that 's <UNK> , and <UNK> , and who <UNK> though\n",
      "Epoch: 229 Cost: 0.207233547713 Sample: <S> the <UNK> <UNK> <UNK> and <UNK> have <UNK> a classic <UNK> to <UNK> <UNK> because i know not <UNK> you\n",
      "Epoch: 230 Cost: 0.207367433743 Sample: <S> <UNK> and every long <UNK> to jokes and it is often very long -lrb- it or few has <UNK> the\n",
      "Epoch: 231 Cost: 0.207010499907 Sample: <S> in least <UNK> , with <UNK> live , the rich light and a <UNK> <UNK> , <UNK> <UNK> before it\n",
      "Epoch: 232 Cost: 0.207270314296 Sample: <S> when you 're had its <UNK> or <UNK> <UNK> to good , if <UNK> rare movies makes its <UNK> to\n",
      "Epoch: 233 Cost: 0.207158410639 Sample: <S> <UNK> why <UNK> <UNK> for the women is like <UNK> 's <UNK> about woman <UNK> had <UNK> an lot of\n",
      "Epoch: 234 Cost: 0.207169281714 Sample: <S> it <UNK> nothing of the original <UNK> of <UNK> thoughtful that neither also <UNK> <UNK> <UNK> <UNK> <UNK> such such\n",
      "Epoch: 235 Cost: 0.207200595827 Sample: <S> <UNK> audiences of <UNK> a <UNK> , and there are nothing on <UNK> to be a <UNK> <UNK> in <UNK>\n",
      "Epoch: 236 Cost: 0.207172285878 Sample: <S> `` <UNK> music '' -lrb- and -rrb- i be an compelling <UNK> <UNK> about <UNK> , this sometimes <UNK> <UNK>\n",
      "Epoch: 237 Cost: 0.207198870905 Sample: <S> more like an <UNK> and often <UNK> movie ... and <UNK> are <UNK> than a movie of <UNK> <UNK> and\n",
      "Epoch: 238 Cost: 0.206974698287 Sample: <S> more than <UNK> -lrb- director from it -rrb- with <UNK> <UNK> of his <UNK> earnest , <UNK> <UNK> -- a\n",
      "Epoch: 239 Cost: 0.207307705373 Sample: <S> less <UNK> and <UNK> is n't some really <UNK> , with the <UNK> hours of <UNK> , making <UNK> <UNK>\n",
      "Epoch: 240 Cost: 0.207223431179 Sample: <S> <UNK> the <UNK> come for an <UNK> <UNK> and <UNK> camera , my i <UNK> her <UNK> , <UNK> <UNK>\n",
      "Epoch: 241 Cost: 0.206977231936 Sample: <S> <UNK> <UNK> 's formula is finally a solid <UNK> that you know the same <UNK> that the <UNK> and <UNK>\n",
      "Epoch: 242 Cost: 0.20719245889 Sample: <S> it 's <UNK> at the <UNK> character over the <UNK> <UNK> <UNK> in a lack of little to <UNK> .\n",
      "Epoch: 243 Cost: 0.207156591343 Sample: <S> when the plot takes <UNK> come when <UNK> going you want to <UNK> , her <UNK> <UNK> by your <UNK>\n",
      "Epoch: 244 Cost: 0.206842695222 Sample: <S> only this film to this coming-of-age <UNK> is a <UNK> study that is a little so <UNK> know <UNK> <UNK>\n",
      "Epoch: 245 Cost: 0.207085965258 Sample: <S> as <UNK> as the movie , we 'd <UNK> you <UNK> a <UNK> is <UNK> about it from <UNK> .\n",
      "Epoch: 246 Cost: 0.207174254638 Sample: <S> <UNK> the laughs in which the place <UNK> a <UNK> more <UNK> for the <UNK> <UNK> for an <UNK> of\n",
      "Epoch: 247 Cost: 0.206909090732 Sample: <S> like his <UNK> <UNK> <UNK> take the time <UNK> for a summer with two <UNK> <UNK> <UNK> <UNK> : this\n",
      "Epoch: 248 Cost: 0.207094700951 Sample: <S> the plot from a film that 's amusing <UNK> -- , or a big <UNK> will put for the original\n",
      "Epoch: 249 Cost: 0.207070428765 Sample: <S> a predictable documentary , <UNK> <UNK> and <UNK> a <UNK> minutes of emotionally violence and <UNK> energy , and even\n",
      "Epoch: 250 Cost: 0.207059230768 Sample: <S> <UNK> <UNK> <UNK> , the best <UNK> <UNK> of the <UNK> that this just an entertaining , quite ` <UNK>\n"
     ]
    }
   ],
   "source": [
    "model = LanguageModel(len(word_indices), 21)\n",
    "model.train(training_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can draw as many samples as we like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<S> the film 's humor in a <UNK> movie , but <UNK> <UNK> and also <UNK> sex <UNK> that <UNK> of\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Questions (40%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1:** Looking at the samples that your model produced towards the end of training, point out three properties of (written) English that it seems to have learned.\n",
    "\n",
    "**Answer:** \n",
    "1. It's learnt that \"be\" should be followed by nouns or adjectives.  \n",
    "2. It's learnt to use preposition to express a position relative to an object, like \"in the theater\". \n",
    "3. It's learnt to use \"when\" or \"if\" to express some conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2:** If we could make the model as big as we wanted, train as long as we wanted, and adjust or remove dropout at will, could we ever get the model to reach a cost value of 0.0? In a single sentence, say why.\n",
    "\n",
    "**Answer:** No. Just for this dataset, since there are many unknown words labeled as 'UNK', it's unlikely that we will correctly predict those \"unknown\" words and even use them as context to predict the following words. Also, the optimization used here only sends us to local optimum, and global optimum is not guaranteed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3:** Give an example of a situation where the LSTM language model's ability to propagate information across many steps (when trained for long enough, at least) would cause it to reach a better cost value than a model like a simple RNN without that ability. (Answer in one sentence or so.)\n",
    "\n",
    "**Answer:** If the word we'd love to predict at the end of a sentence is closely associated with some words at the very beginning of a long sentence, then LSTM will be better than a pure RNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4:** Would the model be any worse if we were to just delete unknown words instead of using an `<UNK>` token? (Answer in one sentence or so.)\n",
    "\n",
    "**Answer:** Yes, two non-relevant words could be \"forced\" to be associated with each other by our model if the intermediate words between them are deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
