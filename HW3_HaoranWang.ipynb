{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building GloVe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, you're going to implement GloVe, one of the more popular, effective, and efficient approaches to learning word embeddings. You should use the [paper](http://nlp.stanford.edu/pubs/glove.pdf) for reference, and you're welcome to look to other implementations for guidance as long as the code that you submit is your own and it fits the basic structure provided by the starter code below.\n",
    "\n",
    "Submit your completed notebook through NYU Classes by 9:30 AM on October 13."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to run GloVe on the text of the Stanford Sentiment Treebank (SST) training set. Usually these methods are run on extremely large corpora, but we're using this here to make sure that you can train a reasonable model without waiting for hours or days. \n",
    "\n",
    "First, let's load the data as before. For our purposes, we won't need either the labels or any of the test or dev data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sst_home = '../trees'\n",
    "\n",
    "import re\n",
    "\n",
    "def load_sst_data(path):\n",
    "    data = []\n",
    "    with open(path) as f:\n",
    "        for i, line in enumerate(f): \n",
    "            example = {}\n",
    "            text = re.sub(r'\\s*(\\(\\d)|(\\))\\s*', '', line)\n",
    "            example['text'] = text[1:]\n",
    "            data.append(example)\n",
    "    return data\n",
    "     \n",
    "training_set = load_sst_data(sst_home + '/train.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's count cooccurrences on the training set. We'll use a nine-word window. Along the way, we'll also collect a list of all of the index pairs $(i,j)$ that have a count greater than zero. \n",
    "\n",
    "To speed up GloVe training below, though, we'll only consider the 250 most frequent words in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(238, 115)\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "\n",
    "def tokenize(string):\n",
    "    return string.split()\n",
    "\n",
    "word_counter = collections.Counter()\n",
    "for example in training_set:\n",
    "    word_counter.update(tokenize(example['text']))\n",
    "vocabulary = [pair[0] for pair in word_counter.most_common()[0:250]]\n",
    "index_to_word_map = dict(enumerate(vocabulary))\n",
    "word_to_index_map = dict([(index_to_word_map[index], index) for index in index_to_word_map])\n",
    "\n",
    "def extract_cooccurrences(dataset, word_map, amount_of_context=4):\n",
    "    num_words = len(vocabulary)\n",
    "    cooccurrences = np.zeros((num_words, num_words))\n",
    "    nonzero_pairs = set()\n",
    "    for example in dataset:\n",
    "        words = tokenize(example['text'])\n",
    "        for target_index in range(len(words)):\n",
    "            target_word = words[target_index]\n",
    "            if target_word not in word_to_index_map:\n",
    "                continue\n",
    "            target_word_index = word_to_index_map[target_word]\n",
    "            min_context_index = max(0, target_index - amount_of_context)\n",
    "            max_word = min(len(words), target_index + amount_of_context)\n",
    "            for context_index in range(min_context_index, target_index) + range(target_index + 1, max_word):\n",
    "                context_word = words[context_index]\n",
    "                if context_word not in word_to_index_map:\n",
    "                    continue\n",
    "                context_word_index = word_to_index_map[context_word]\n",
    "                cooccurrences[target_word_index][context_word_index] += 1.0\n",
    "                nonzero_pairs.add((target_word_index, context_word_index))\n",
    "    return cooccurrences, list(nonzero_pairs)\n",
    "                \n",
    "cooccurrences, nonzero_pairs = extract_cooccurrences(training_set, vocabulary)\n",
    "print list(nonzero_pairs)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Implementation (60%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up evalation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be frank, a GloVe model trained on such a small dataset and vocabulary won't be spectacular, so we won't bother with a full-fledged similarity or analogy evaluation. Instead, we'll use the simple scoring function below, which grades the model on how well it captures ten easy/simple similarity comparisons. The function returns a score between 0 and 10. Random embeddings can be expected to get a score of 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def similarity(word_one, word_two):\n",
    "    vec_one = model.get_embedding(word_to_index_map[word_one]).reshape(1, -1)\n",
    "    vec_two = model.get_embedding(word_to_index_map[word_two]).reshape(1, -1)\n",
    "    return float(cosine_similarity(vec_one, vec_two))\n",
    "\n",
    "def score(model):\n",
    "    score = 0\n",
    "    score += similarity('a', 'an') > similarity('a', 'documentary')\n",
    "    score += similarity('in', 'of') > similarity('in', 'picture')\n",
    "    score += similarity('action', 'thriller') >  similarity('action', 'end')\n",
    "    score += similarity('films', 'movies') > similarity('films', 'almost')\n",
    "    score += similarity('film', 'movie') > similarity('film', 'movies')\n",
    "    score += similarity('script', 'plot') > similarity('script', 'big')\n",
    "    score += similarity('watch', 'see') > similarity('watch', 'down')\n",
    "    score += similarity('``', \"''\") > similarity('``', 'quite')\n",
    "    score += similarity('funny', 'entertaining') > similarity('funny', 'seems')\n",
    "    score += similarity('good', 'great') > similarity('good', 'minutes')\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you've built and trained the model, you can evaluate it as follows. You may not have to, though, since evaluation is built into the model code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-006c66a3fe05>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "score(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the model (30%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fill it out to create an implementation of GloVe, then train it on the SST training set.**\n",
    "\n",
    "Some tips:\n",
    "\n",
    "- You should use minibatch SGD (the starter code is set up for it), and you should run computation for an entire minibatch at a time using a single `sess.run()` call. This means that many of your variables will have an extra batch dimension in addition to the dimensions that you'd expect from reading the paper. \n",
    "- Every time you define any new TF computation, add a comment indicating what shape (/dimensions) you expect the result to be. Use `tf.Print()` and `tf.shape()` to make sure that you're getting what you expect.\n",
    "- You'll likely need to use `tf.reshape()` and `tf.batch_matmul()` at least once each.\n",
    "- If you're new to TF, try to find a partner or group to work with. The solution is fairly simple, but finding it may not be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import random\n",
    "\n",
    "class glove:\n",
    "    def __init__(self, num_words):\n",
    "        # Define the hyperparameters\n",
    "        self.dim = 10                # The size of the learned embeddings\n",
    "        self.alpha = 0.75            # One of the hyperparameters defining the scaling function F\n",
    "        self.xmax = 50               # One of the hyperparameters defining the scaling function F \n",
    "        self.learning_rate = 0.7     # SGD LR - Much higher than you'll see for typical NNs, but it works here\n",
    "        self.batch_size = 1024       # Somewhat arbitrary - can be tuned, but often tuned for speed, not accuracy\n",
    "        self.training_epochs = 2100  # This model should train faster per epoch than the logistic regression models,\n",
    "                                     # so we can afford to run for more epochs. You should feel free to stop the model\n",
    "                                     # during training if it seems to stop improving.\n",
    "        self.display_epoch_freq = 25 # How often to test and print out statistics\n",
    "        self.num_words = num_words   # The number of vectors to learn\n",
    "        self.embeddings = None       # To be set later\n",
    "    \n",
    "        # Define the inputs to the model\n",
    "        self.target_index = tf.placeholder(tf.int32, shape=[self.batch_size])\n",
    "        self.context_index = tf.placeholder(tf.int32, shape=[self.batch_size])\n",
    "        self.cooccurrences = tf.placeholder(tf.float32, shape=[self.batch_size])\n",
    "        self.count_max = tf.constant([self.xmax], dtype=tf.float32)\n",
    "        self.scaling_factor = tf.constant([self.alpha], dtype=tf.float32)\n",
    "        \n",
    "        # Define the trainable parameters of the model\n",
    "        self.target_embeddings = tf.Variable(tf.random_normal([self.num_words, self.dim],0,0.1))\n",
    "        self.context_embeddings = tf.Variable(tf.random_normal([self.num_words, self.dim],0,0.1))\n",
    "        self.target_biases = tf.Variable(tf.random_normal([self.num_words],0,0.1))\n",
    "        self.context_biases = tf.Variable(tf.random_normal([self.num_words],0,0.1))\n",
    "        \n",
    "        # parallel look-ups for embeddings\n",
    "        self.target_embedding = tf.nn.embedding_lookup([self.target_embeddings], self.target_index)\n",
    "        self.context_embedding = tf.nn.embedding_lookup([self.context_embeddings], self.context_index)\n",
    "        self.target_bias = tf.nn.embedding_lookup([self.target_biases], self.target_index)\n",
    "        self.context_bias = tf.nn.embedding_lookup([self.context_biases], self.context_index)\n",
    "        self.F = tf.minimum(1.0, tf.pow(tf.div(self.cooccurrences, self.count_max), \n",
    "                                                       self.scaling_factor))\n",
    "        \n",
    "        # Define the forward computation of the model\n",
    "        # The final result that you compute should be a 1024-dimensional vector of example-by-example \n",
    "        # cost function values called `self.example_cost`.\n",
    "        self.target_context_mult = tf.reduce_sum(tf.mul(self.target_embedding, \n",
    "                                                        self.context_embedding), 1) # sum over axis 1\n",
    "        self.square = tf.square(tf.add_n([self.target_context_mult, \n",
    "                                          self.target_bias, \n",
    "                                          self.context_bias, \n",
    "                                          tf.neg(tf.log(tf.to_float(self.cooccurrences)))]))\n",
    "\n",
    "        self.example_cost = tf.mul(self.F, self.square)\n",
    "        # Define the cost function\n",
    "        self.total_cost = tf.reduce_mean(self.example_cost)\n",
    "        \n",
    "        # This library call performs the main SGD update equation\n",
    "        self.optimizer = tf.train.GradientDescentOptimizer(self.learning_rate).minimize(self.total_cost)\n",
    "        self.embeddings = tf.add(self.target_embeddings, self.context_embeddings)\n",
    "        \n",
    "        # Create an operation to fill zero values in for W and b\n",
    "        self.init = tf.initialize_all_variables()\n",
    "        \n",
    "        # Initialize the model\n",
    "        self.sess = tf.Session()\n",
    "        \n",
    "        self.sess.run(self.init)\n",
    "        \n",
    "    def train(self, cooccurrences, nonzero_pairs, num_words):\n",
    "        self.embeddings = None  # If we restart training, make sure to clear the cached embeddings\n",
    "        print 'Training.'\n",
    "        \n",
    "        # Training cycle - In one epoch, we'll visit each nonzero entry in cooccurrences once\n",
    "        for epoch in range(self.training_epochs):\n",
    "            random.shuffle(nonzero_pairs)\n",
    "\n",
    "            avg_cost = 0.\n",
    "            total_batches = int(len(nonzero_pairs) / self.batch_size)\n",
    "            \n",
    "            # Loop over all batches in epoch\n",
    "            for i in range(total_batches):\n",
    "                # Assemble a minibatch dictionary to feed to `sess.run()`\n",
    "                feed = {}\n",
    "                # randomly sample self.batch_size examples from the data\n",
    "                pairs = list(nonzero_pairs)[i*self.batch_size: (i+1)*self.batch_size]\n",
    "                target_indexes = []\n",
    "                context_indexes = []\n",
    "                cooccur = []\n",
    "                for pair in pairs:\n",
    "                    target_indexes.append(pair[0])\n",
    "                    context_indexes.append(pair[1])\n",
    "                    cooccur.append(cooccurrences[pair[0]][pair[1]])\n",
    "                feed = {self.target_index: target_indexes, \n",
    "                        self.context_index: context_indexes, \n",
    "                        self.cooccurrences: cooccur}\n",
    "                # Run the optimizer to take a gradient step, and also fetch the value of the \n",
    "                # cost function for logging\n",
    "                _, c = self.sess.run([self.optimizer, self.total_cost], \n",
    "                                     feed_dict=feed)                                    \n",
    "                # Compute average loss\n",
    "                avg_cost += c / total_batches\n",
    "                \n",
    "            # Display some statistics about the step\n",
    "            if (epoch+1) % self.display_epoch_freq == 0:\n",
    "                self.cache_embeddings()  # Make sure we run scoring with a fresh copy of the embeddings\n",
    "                print \"Epoch:\", (epoch+1), \"Cost:\", avg_cost, \"Score:\", score(self)\n",
    "\n",
    "    def cache_embeddings(self):\n",
    "        # Fill in self.embeddings with a matrix (in NumPy format) containing one vector per word, \n",
    "        # representing the final output of GloVe. It should be possible to index into that\n",
    "        # matrix using `get_embedding` below.\n",
    "        # self.embeddings = np.zeros((self.num_words, self.dim))\n",
    "        \n",
    "        self.embeddings = tf.add(self.target_embeddings, self.context_embeddings)\n",
    "        \n",
    "        \n",
    "    def get_embedding(self, index):\n",
    "        if self.embeddings is None:\n",
    "            cache_embeddings()\n",
    "        return self.embeddings.eval(session=self.sess)[index, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a working model (30%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should use the following commands to train the model for at least 2000 steps. If your model works, it will usually converge to a score of 10 within that many steps.\n",
    "\n",
    "Tips:\n",
    "\n",
    "- You cannot run this until you've completed the previous code block. That's because the starter code doesn't actually define at trainable model.\n",
    "- The score and cost for the first few hundred epochs of training will vary quite a bit due to the random initialization. The real measure of success is how the model does once it nears convergence.\n",
    "- Make sure to show the full output for a real run in your notebook when you submit. I will not retrain your model to grade it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = glove(len(vocabulary))  # Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training.\n",
      "Epoch: 25 Cost: 0.100168309428 Score: 2\n",
      "Epoch: 50 Cost: 0.0944384020386 Score: 4\n",
      "Epoch: 75 Cost: 0.0924470885233 Score: 4\n",
      "Epoch: 100 Cost: 0.0912781556447 Score: 4\n",
      "Epoch: 125 Cost: 0.0892373210553 Score: 7\n",
      "Epoch: 150 Cost: 0.0865125671932 Score: 7\n",
      "Epoch: 175 Cost: 0.0833432003856 Score: 7\n",
      "Epoch: 200 Cost: 0.0801719409047 Score: 9\n",
      "Epoch: 225 Cost: 0.0770528486958 Score: 10\n",
      "Epoch: 250 Cost: 0.0743092187878 Score: 10\n",
      "Epoch: 275 Cost: 0.0717547635237 Score: 10\n",
      "Epoch: 300 Cost: 0.0693541609428 Score: 10\n",
      "Epoch: 325 Cost: 0.0670668171211 Score: 10\n",
      "Epoch: 350 Cost: 0.0651142620905 Score: 10\n",
      "Epoch: 375 Cost: 0.063206298559 Score: 10\n",
      "Epoch: 400 Cost: 0.0615662512454 Score: 10\n",
      "Epoch: 425 Cost: 0.0600534141289 Score: 10\n",
      "Epoch: 450 Cost: 0.0589321727554 Score: 10\n",
      "Epoch: 475 Cost: 0.0578365049353 Score: 10\n",
      "Epoch: 500 Cost: 0.0567841970108 Score: 10\n",
      "Epoch: 525 Cost: 0.0558440400796 Score: 10\n",
      "Epoch: 550 Cost: 0.0550860566172 Score: 10\n",
      "Epoch: 575 Cost: 0.0544035652596 Score: 10\n",
      "Epoch: 600 Cost: 0.0537403070114 Score: 10\n",
      "Epoch: 625 Cost: 0.0530646505455 Score: 10\n",
      "Epoch: 650 Cost: 0.0524584949017 Score: 10\n",
      "Epoch: 675 Cost: 0.0520658323711 Score: 10\n",
      "Epoch: 700 Cost: 0.0516137722315 Score: 10\n",
      "Epoch: 725 Cost: 0.0512355585216 Score: 10\n",
      "Epoch: 750 Cost: 0.0508618038712 Score: 10\n",
      "Epoch: 775 Cost: 0.0504644357345 Score: 10\n",
      "Epoch: 800 Cost: 0.050126187842 Score: 10\n",
      "Epoch: 825 Cost: 0.0497841130603 Score: 10\n",
      "Epoch: 850 Cost: 0.0495354449659 Score: 10\n",
      "Epoch: 875 Cost: 0.0492849739438 Score: 10\n",
      "Epoch: 900 Cost: 0.0489546827069 Score: 10\n",
      "Epoch: 925 Cost: 0.0488407908741 Score: 10\n",
      "Epoch: 950 Cost: 0.0485965565524 Score: 10\n",
      "Epoch: 975 Cost: 0.0483747413664 Score: 10\n",
      "Epoch: 1000 Cost: 0.0481786634209 Score: 10\n",
      "Epoch: 1025 Cost: 0.0479926758192 Score: 10\n",
      "Epoch: 1050 Cost: 0.0479124247125 Score: 10\n",
      "Epoch: 1075 Cost: 0.0476949365076 Score: 10\n",
      "Epoch: 1100 Cost: 0.0476046943981 Score: 10\n",
      "Epoch: 1125 Cost: 0.0473772754723 Score: 10\n",
      "Epoch: 1150 Cost: 0.0472527559508 Score: 10\n",
      "Epoch: 1175 Cost: 0.0471154223337 Score: 10\n",
      "Epoch: 1200 Cost: 0.0470698251191 Score: 10\n",
      "Epoch: 1225 Cost: 0.0469157109884 Score: 10\n",
      "Epoch: 1250 Cost: 0.0467980237621 Score: 10\n",
      "Epoch: 1275 Cost: 0.0467727648264 Score: 10\n",
      "Epoch: 1300 Cost: 0.0464760584362 Score: 10\n",
      "Epoch: 1325 Cost: 0.0465480913267 Score: 10\n",
      "Epoch: 1350 Cost: 0.0464135614986 Score: 10\n",
      "Epoch: 1375 Cost: 0.046388817556 Score: 10\n",
      "Epoch: 1400 Cost: 0.0462549214788 Score: 10\n",
      "Epoch: 1425 Cost: 0.046130995633 Score: 10\n",
      "Epoch: 1450 Cost: 0.0461624712881 Score: 10\n",
      "Epoch: 1475 Cost: 0.0460832578441 Score: 10\n",
      "Epoch: 1500 Cost: 0.046026550572 Score: 10\n",
      "Epoch: 1525 Cost: 0.0459168052131 Score: 10\n",
      "Epoch: 1550 Cost: 0.0459021868354 Score: 10\n",
      "Epoch: 1575 Cost: 0.0458242200089 Score: 10\n",
      "Epoch: 1600 Cost: 0.045771111706 Score: 10\n",
      "Epoch: 1625 Cost: 0.0456830154766 Score: 10\n",
      "Epoch: 1650 Cost: 0.0457176974777 Score: 10\n",
      "Epoch: 1675 Cost: 0.0455485582352 Score: 10\n",
      "Epoch: 1700 Cost: 0.0455554661651 Score: 10\n",
      "Epoch: 1725 Cost: 0.0455107592949 Score: 10\n",
      "Epoch: 1750 Cost: 0.0455074488665 Score: 10\n",
      "Epoch: 1775 Cost: 0.0454461861289 Score: 10\n",
      "Epoch: 1800 Cost: 0.0454332042824 Score: 10\n",
      "Epoch: 1825 Cost: 0.0453707238264 Score: 10\n",
      "Epoch: 1850 Cost: 0.045350602066 Score: 10\n",
      "Epoch: 1875 Cost: 0.0451847444655 Score: 10\n",
      "Epoch: 1900 Cost: 0.045313476726 Score: 10\n",
      "Epoch: 1925 Cost: 0.0452677197754 Score: 10\n",
      "Epoch: 1950 Cost: 0.0452429497558 Score: 10\n",
      "Epoch: 1975 Cost: 0.0452114770358 Score: 10\n",
      "Epoch: 2000 Cost: 0.0452088035192 Score: 10\n",
      "Epoch: 2025 Cost: 0.0451140374397 Score: 10\n",
      "Epoch: 2050 Cost: 0.045116026406 Score: 10\n",
      "Epoch: 2075 Cost: 0.0449954150527 Score: 10\n",
      "Epoch: 2100 Cost: 0.0450800833377 Score: 10\n"
     ]
    }
   ],
   "source": [
    "model.train(cooccurrences, nonzero_pairs, len(vocabulary))  # Train it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Questions (40%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in your answers below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1:** What do the entries on the diagonal of the `cooccurrences` matrix represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** The diagonal entries of the matrix represent how many times a word appears withint the context of itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2:** Deleting the weighting function $F$ should hurt the performance of the model. Why would you expect this to be the case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** If the function $F$ is deleted, then it means that all of the cooccurrences are weighed equally. If some of the cooccurrences are very rare, then they actually give little information for our task here so they can be regarded as noise in some sense. So it's better to mitigate the effect of the noise by giving them smaller weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3:** What would you expect to happen if you used a learning rate of 0.0001. Why? (You're welcome to try it, but that shouldn't be the sole basis for your answer.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** The convergence rate will be very slow, because the step size for gradient descent now is very small. But sometimes it might be possible that it will eventually give a better optimality than much larger rate like 1.0 even though it will take a much longer time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4:** If you used 100 times more training data, would the model take 100 times as long to train? Just as long as now? Somewhere in between? Choose one and (informally) defend your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** The running time will become somewhere in between. When we have 100 times more training data, we can expect that the variety of the data is greater than before, so the direction of gradient will be more \"accurate\" in the sense that it will be a better approximate of the global gradient. Also, with more data, the estimate of co-occurrence matrix will be more accurate as well according to the \"law of large numbers\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
