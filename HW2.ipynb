{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW2: Naïve Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this homework, you'll implement and (very briefly) discuss a bag-of-words Naïve Bayes sentiment classifier—a simple but effective example of a linear classifier.\n",
    "\n",
    "This assignment is due at the start of class on September 29. When you're done, upload your edited `ipynb` file to NYU Classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's load the Stanford Sentiment Treebank. If you don't already have it, download it from here: [the train/dev/test Stanford Sentiment Treebank distribution](http://nlp.stanford.edu/sentiment/trainDevTestTrees_PTB.zip), unzip it, and put the resulting folder in the same directory as this notebook. (If you want to put it somewhere else, change `sst_home` below.)\n",
    "\n",
    "Note: Unlike with k-nearest neighbors, Naïve Bayes evaluation should be quite fast (thousands of examples per second at least), so we don't need to trim down the dev and test sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': \"And if you 're not nearly moved to tears by a couple of scenes , you 've got ice water in your veins .\", 'label': 1}\n"
     ]
    }
   ],
   "source": [
    "sst_home = './trees'\n",
    "\n",
    "import re\n",
    "import random\n",
    "\n",
    "def load_sst_data(path):\n",
    "    \n",
    "    EASY_LABEL_MAP = {0:0, 1:0, 2:None, 3:1, 4:1}\n",
    "    \n",
    "    data = []\n",
    "    with open(path) as f:\n",
    "        for i, line in enumerate(f): \n",
    "            example = {}\n",
    "            example['label'] = EASY_LABEL_MAP[int(line[1])]\n",
    "            if example['label'] is None:\n",
    "                continue\n",
    "            \n",
    "            # Strip out the parse information and the phrase labels\n",
    "            text = re.sub(r'\\s*(\\(\\d)|(\\))\\s*', '', line)\n",
    "            example['text'] = text[1:]\n",
    "            data.append(example)\n",
    "\n",
    "    return data\n",
    "     \n",
    "training_set = load_sst_data(sst_home + '/train.txt')\n",
    "dev_set = load_sst_data(sst_home + '/dev.txt')\n",
    "test_set = load_sst_data(sst_home + '/test.txt')\n",
    "\n",
    "print dev_set[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Bags of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's write a function to convert these sentences into feature vectors. The function template here simply extracts three useless (?) dummy features:\n",
    "\n",
    "- The number of characters in the review.\n",
    "- The first letter in the review.\n",
    "- Whether the letters 'th' appear in the review.\n",
    "\n",
    "This function depends upon a simple dictionary trick that allows us to reason about features by name rather than by index.\n",
    "\n",
    "For this classifier, we'll be sticking to bag-of-words features. Delete the existing features, and replace them with bag-of-words features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def feature_function(datasets):\n",
    "    ''' A function that converts raw text into bag-of-words features\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    datasets: A list where each entry is one observation, represented as \n",
    "        a dictionary with \"text\" and \"label\" being keys.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    result: A list where each entry is one observation, represented\n",
    "        as a dictionary with \"feature\" and \"label\" being keys. The value of \n",
    "        \"feature\" is a dictionary represented in the format of bag-of-words.\n",
    "    class_words_occurrences: A dictionary with keys being the class labels\n",
    "        and value being the number of corresponding words occurrences\n",
    "    '''\n",
    "    # a list containing the transformed observations in the datasets\n",
    "    result = []\n",
    "    # a counter that counts the total number of word occurrences of each class\n",
    "    class_counter = Counter()\n",
    "    # a set containing the vocabulary for the document\n",
    "    vocabulary = set()\n",
    "    for obs in datasets:\n",
    "        # get bag of words features\n",
    "        obs_bag_of_words = extract_features(obs)\n",
    "        result.append(obs_bag_of_words)\n",
    "        # update the vocabulary\n",
    "        vocabulary.update(obs_bag_of_words['feature'].keys())\n",
    "        # update the total number of word occurrences for each class\n",
    "        class_counter[obs[\"label\"]] += np.sum(obs_bag_of_words['feature'].values())\n",
    "    class_words_occurrences = dict(class_counter)\n",
    "    num_words = len(vocabulary)\n",
    "    return result, class_words_occurrences, num_words\n",
    "\n",
    "def extract_features(observation):\n",
    "    ''' Extract bag-of-words features from the raw observation represented\n",
    "    as a dictionary\n",
    "    '''\n",
    "    list_of_words = observation[\"text\"].split(\" \")\n",
    "    # get bag of words features\n",
    "    bag_of_words = count(list_of_words)\n",
    "    return {\"feature\": bag_of_words, \"label\": observation[\"label\"]}\n",
    "    \n",
    "def count(words):\n",
    "    ''' A function that counts the number of occurences of words in a list\n",
    "    Returns a dictionary representing the words and their occurrences\n",
    "    '''\n",
    "    counter = Counter()\n",
    "    counter.update(words)\n",
    "    return dict(counter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Implementing Naïve Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, implement a Naïve Bayes classifier that you can train and test on the feature vectors you just extracted. Use Laplace (add-one) smoothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class NaiveBayesClassifier:\n",
    "    ''' A naive bayes classifier using Laplace smoothing\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        # initiate some variables for training and classifying\n",
    "        self.log_prior = {} # class priors as log probability\n",
    "        self.log_likelihood = {} # likelihood terms of words as log prob\n",
    "        self.class_words_occurrences = {} # number of word occurrences for each class\n",
    "        self.num_words = 0 # total number of words in the vocabulary of the doc\n",
    "        self.trained = False # already trained or not\n",
    "        \n",
    "    def train(self, training_set):\n",
    "        # get the observation represented as bag-of-words, \n",
    "        # the number of all the word occurrences for each class,\n",
    "        # and the number of words in the vocabulary of training set\n",
    "        bag_of_words, class_words_occurrences, num_words = (\n",
    "            feature_function(training_set))\n",
    "        self.class_words_occurrences = class_words_occurrences\n",
    "        self.num_words = num_words\n",
    "        # count the number of observations for each class, to \n",
    "        # be used later to calculate the prior of classes\n",
    "        label_counter = Counter()\n",
    "        # the number of occurrences of each word for each class\n",
    "        counters = {}\n",
    "        \n",
    "        # initiate a dictionary to store the counters for each class\n",
    "        for class_label in class_words_occurrences:\n",
    "            counters[class_label] = Counter()\n",
    "        # iterate through through all the observations\n",
    "        for obs in bag_of_words:\n",
    "            label = obs['label']\n",
    "            label_counter[label] += 1  # update the count of documents for this class\n",
    "            features = obs['feature']\n",
    "            counters[label].update(features)  # update the count of each word for this class\n",
    "\n",
    "        # update self.prior and self.log_likelihood\n",
    "        for class_label in class_words_occurrences:\n",
    "            # calculate the class prior\n",
    "            self.log_prior[class_label] = (np.log(1.0*label_counter[class_label]\n",
    "                                                  /len(training_set)))\n",
    "            # calculate the log likelihood of each word for each class\n",
    "            # using Laplace smoothing\n",
    "            log_likelihood = {}\n",
    "            for word in counters[class_label]:\n",
    "                log_likelihood[word] = (np.log(1.0*(counters[class_label][word] + 1)/\n",
    "                                               (class_words_occurrences[class_label] + num_words)))\n",
    "            self.log_likelihood[class_label] = log_likelihood\n",
    "        self.trained = True\n",
    "\n",
    "    def classify(self, example):\n",
    "        # chech if it's already trained\n",
    "        if self.trained == False:\n",
    "            raise Exception(\"Please train the classifier first.\")\n",
    "        # extract bag-of-words features\n",
    "        bag_of_words_obs = extract_features(example)\n",
    "        max_log_sum = -float('inf')\n",
    "        hypothesis = 0\n",
    "        for label in self.log_prior:\n",
    "            log_sum = self.log_prior[label]\n",
    "            for word in bag_of_words_obs['feature']:\n",
    "                # if the word has not appreared\n",
    "                if not word in self.log_likelihood[label]: # use 0+1 as the occurrence of the word\n",
    "                    log_sum += (np.log(1.0/(self.class_words_occurrences[label] + \n",
    "                                            self.num_words)))*bag_of_words_obs['feature'][word]\n",
    "                else:\n",
    "                    log_sum += (self.log_likelihood[label][word]*\n",
    "                                bag_of_words_obs['feature'][word])\n",
    "            # get the hypothesis with the maximum log sum probability\n",
    "            if log_sum > max_log_sum:\n",
    "                max_log_sum = log_sum\n",
    "                hypothesis = label\n",
    "        return hypothesis\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how it's trained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classifier = NaiveBayesClassifier()\n",
    "classifier.train(training_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how it's called. It returns a label (0 for negative, 1 for positive):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print classifier.classify(dev_set[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a simple function to evaluate a classifier. It expects a function from example to labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def evaluate_classifier(classifier, eval_set):\n",
    "    correct = 0\n",
    "    for example in eval_set:\n",
    "        hypothesis = classifier.classify(example)\n",
    "        if hypothesis == example['label']:\n",
    "            correct += 1\n",
    "    return correct / float(len(eval_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This runs the primary evaluation. It'll return accuracy (%). If you've implemented Naïve Bayes correctly, you should see accuracy of greater than 75% on the dev set. The [original Stanford Sentiment Treebank paper](http://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf) reports 81.8% test accuracy with Naïve Bayes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.792431192661\n"
     ]
    }
   ],
   "source": [
    "print evaluate_classifier(classifier, dev_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Briefly answer each of the questions below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1:** Most implementations of Naïve Bayes (hopefully including yours), never actually compute $P(d|c)$, but instead directly compute $\\log P(d|c)$. Why is this?\n",
    "\n",
    "**Answer:** \n",
    "1. Since the data matrix can be very sparse, so it's possible that each P(d|c) can be very small, and when we multiply theses independent P's tegether, the product can be so extremely small that the computer cannot accurately represent it with complete precision, which causes underflow. So instead of computing the extremely small probability, we do log probability so that we don't need to care about the precision of extremely small numbers in computer. \n",
    "2. By converting multiplication to summation, hopefully the calculation process can be relatively faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2:** In class, we found that a nearest neighbor model built over bag-of-words features barely surpassed 60% accuracy on the dev set. Why is Naïve Bayes so much better at classifying sentences using this same style of feature?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "Theoretically, by directly applying KNN to high dimension data, we get \"curse of dimensionality\". Since we are using bag of words approach so the dimension can be very high and thus the issue of curse of dimensionality can be quite serious. One critical way to overcome this type of \"curse\" is to make assumptions about the nature of data distribution. And these assumptions are inherently involved in some parametric models. Since Naive Bayes is essentially a linear classifier and we have very strong assumptions of independence, so the \"curse of dimensionality\" can be mitigated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3:** Do some error analysis---identify three sentences that the model mis-classified, and speculate about ways in which a better (but still realistic) machine learning model trained on this same training set might be able to do better.\n",
    "\n",
    "**Answer:**\n",
    "From the error analysis below, we can find that the examples misclassified are usually involve both \"good\" words and \"bad\" words. What discriminate good and bad apart in those sentences are not only just dependent on the words, but more dependent on the sementics of the sentence. I guess we can do a little bit feature engineering to deal with this issue, by combining successive words together as an one feature, which means that we can do 2-grams, 3-grams or so. For example, \"beautiful, magnificant\" can be regarged as \"good\" indicators by our original model but if we consider the potential \"not\" before them, then \"not beautiful, magnificant\" can totally indicates that it's a negative sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': \"A broad , melodramatic estrogen opera that 's pretty toxic in its own right .\", 'label': 0}\n",
      "{'text': \"All that 's missing is the spontaneity , originality and delight .\", 'label': 0}\n",
      "{'text': 'Scores no points for originality , wit , or intelligence .', 'label': 0}\n",
      "{'text': \"I 've always dreamed of attending Cannes , but after seeing this film , it 's not that big a deal .\", 'label': 0}\n",
      "{'text': \"What 's surprising about Full Frontal is that despite its overt self-awareness , parts of the movie still manage to break past the artifice and thoroughly engage you .\", 'label': 1}\n"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "np.random.shuffle(dev_set)\n",
    "for example in dev_set:\n",
    "    hypothesis = classifier.classify(example)\n",
    "    if hypothesis != example['label']:\n",
    "        cnt += 1\n",
    "        print example\n",
    "        if cnt == 5:\n",
    "            break\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
